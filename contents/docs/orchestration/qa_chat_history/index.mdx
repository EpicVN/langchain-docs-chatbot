---
title: Build a Retrieval Augmented Generation (RAG) App Part 2
description: In many Q&A applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of “memory” of past questions and answers, and some logic for incorporating those into its current thinking.
---

This is a the second part of a multi-part tutorial:

- [Part 1](/docs/tutorials/rag) introduces RAG and walks through a minimal implementation.
- Part 2 (this guide) extends the implementation to accommodate conversation-style interactions and multi-step retrieval processes.

Here we focus on adding logic for incorporating historical messages. This involves the management of a chat history.

We will cover two approaches:

1. [Chains](/#agents), in which we execute at most one retrieval step;
2. [Agents](/#agents), in which we give an LLM discretion to execute multiple retrieval steps.

<Note title="NOTE" type="note">
  The methods presented here leverage tool-calling capabilities in modern chat
  models. See this page for a table of models supporting tool calling features.
</Note>

For the external knowledge source, we will use the same [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post by Lilian Weng from the [Part 1](/docs/tutorials/rag) of the RAG tutorial.

## Setup

### Components

We will need to select three components from LangChain’s suite of integrations.

A chat model:

#### Pick your chat model:

Install dependencies:

<Note title="TIP" type="success">
  See [this section for general instructions on installing integration
  packages](/docs/introduction/installation#installation).
</Note>

<Tabs defaultValue="openai" className="pt-5 pb-1">
  <TabsList className="">
    <TabsTrigger value="openai">OpenAI</TabsTrigger>
    <TabsTrigger value="anthropic">Anthropic</TabsTrigger>
    <TabsTrigger value="fireworksai">FireworksAI</TabsTrigger>
    <TabsTrigger value="mistralai">MistralAI</TabsTrigger>
    <TabsTrigger value="groq">Groq</TabsTrigger>
    <TabsTrigger value="vertexai">VertexAI</TabsTrigger>
  </TabsList>

  <TabsContent value="openai">
    ```bash
    npm i @langchain/openai
    ```

    Add environment variables

    ```bash
    OPENAI_API_KEY=your-api-key
    ```

    Instantiate the model

    ```jsx {0} showLineNumbers
    import { ChatOpenAI } from "@langchain/openai";

    const llm = new ChatOpenAI({
    model: "gpt-4o-mini",
    temperature: 0
    });
    ````

  </TabsContent>
  
  <TabsContent value="anthropic">
    ```bash
    npm i @langchain/anthropic
    ```

    Add environment variables

    ```bash
    ANTHROPIC_API_KEY=your-api-key
    ```

    Instantiate the model

    ```jsx {0} showLineNumbers
    import { ChatAnthropic } from "@langchain/anthropic";

    const llm = new ChatAnthropic({
    model: "claude-3-5-sonnet-20240620",
    temperature: 0
    });
    ````

  </TabsContent>

  <TabsContent value="fireworksai">
    ```bash
    npm i @langchain/community
    ```

    Add environment variables

    ```bash
    FIREWORKS_API_KEY=your-api-key
    ```

    Instantiate the model

    ```jsx {0} showLineNumbers
    import { ChatFireworks } from "@langchain/community/chat_models/fireworks";

    const llm = new ChatFireworks({
    model: "accounts/fireworks/models/llama-v3p1-70b-instruct",
    temperature: 0
    });
    ````

  </TabsContent>

  <TabsContent value="mistralai">
    ```bash
    npm i @langchain/mistralai
    ```

    Add environment variables

    ```bash
    MISTRAL_API_KEY=your-api-key
    ```

    Instantiate the model

    ```jsx {0} showLineNumbers
    import { ChatMistralAI } from "@langchain/mistralai";

    const llm = new ChatMistralAI({
    model: "mistral-large-latest",
    temperature: 0
    });
    ````

  </TabsContent>

  <TabsContent value="groq">
    ```bash
    npm i @langchain/groq
    ```

    Add environment variables

    ```bash
    GROQ_API_KEY=your-api-key
    ```

    Instantiate the model

    ```jsx {0} showLineNumbers
    import { ChatGroq } from "@langchain/groq";

    const llm = new ChatGroq({
    model: "mixtral-8x7b-32768",
    temperature: 0
    });
    ````

  </TabsContent>

  <TabsContent value="vertexai">
    ```bash
    npm i @langchain/google-vertexai
    ```

    Add environment variables

    ```bash
    GOOGLE_APPLICATION_CREDENTIALS=credentials.json
    ```

    Instantiate the model

    ```jsx {0} showLineNumbers
    import { ChatVertexAI } from "@langchain/google-vertexai";

    const llm = new ChatVertexAI({
    model: "gemini-1.5-flash",
    temperature: 0
    });
    ````

  </TabsContent>
</Tabs>

An embedding model:

#### Pick your embedding model:

Install dependencies

<Tabs defaultValue="openai" className="pt-5 pb-1">
  <TabsList className="">
    <TabsTrigger value="openai">OpenAI</TabsTrigger>
    <TabsTrigger value="azure">Azure</TabsTrigger>
    <TabsTrigger value="aws">AWS</TabsTrigger>
    <TabsTrigger value="vertexai">VertexAI</TabsTrigger>
    <TabsTrigger value="mistralai">MistralAI</TabsTrigger>
    <TabsTrigger value="cohere">Cohere</TabsTrigger>
  </TabsList>

  <TabsContent value="openai">
    ```bash
    npm i @langchain/openai
    ```

    Add environment variables

    ```bash
    OPENAI_API_KEY=your-api-key
    ```

    Instantiate the model

    ```jsx {0} showLineNumbers
    import { OpenAIEmbeddings } from "@langchain/openai";

    const embeddings = new OpenAIEmbeddings({
    model: "text-embedding-3-large"
    });
    ````

  </TabsContent>
  
  <TabsContent value="azure">
    ```bash
    npm i @langchain/openai
    ```

    Add environment variables

    ```bash
    AZURE_OPENAI_API_INSTANCE_NAME=<YOUR_INSTANCE_NAME>
    AZURE_OPENAI_API_KEY=<YOUR_KEY>
    AZURE_OPENAI_API_VERSION="2024-02-01"
    ```

    Instantiate the model

    ```jsx {0} showLineNumbers
    import { AzureOpenAIEmbeddings } from "@langchain/openai";

    const embeddings = new AzureOpenAIEmbeddings({
    azureOpenAIApiEmbeddingsDeploymentName: "text-embedding-ada-002"
    });
    ````

  </TabsContent>

  <TabsContent value="aws">
    ```bash
    npm i @langchain/aws
    ```

    Add environment variables

    ```bash
    BEDROCK_AWS_REGION=your-region
    ```

    Instantiate the model

    ```jsx {0} showLineNumbers
    import { BedrockEmbeddings } from "@langchain/aws";

    const embeddings = new BedrockEmbeddings({
    model: "amazon.titan-embed-text-v1"
    });
    ````

  </TabsContent>

  <TabsContent value="vertexai">
    ```bash
    npm i @langchain/google-vertexai
    ```

    Add environment variables

    ```bash
    GOOGLE_APPLICATION_CREDENTIALS=credentials.json
    ```

    Instantiate the model

    ```jsx {0} showLineNumbers
    import { VertexAIEmbeddings } from "@langchain/google-vertexai";

    const embeddings = new VertexAIEmbeddings({
    model: "text-embedding-004"
    });
    ````

  </TabsContent>

  <TabsContent value="mistralai">
    ```bash
    npm i @langchain/mistralai
    ```

    Add environment variables

    ```bash
    MISTRAL_API_KEY=your-api-key
    ```

    Instantiate the model

    ```jsx {0} showLineNumbers
    import { MistralAIEmbeddings } from "@langchain/mistralai";

    const embeddings = new MistralAIEmbeddings({
    model: "mistral-embed"
    });
    ````

  </TabsContent>

  <TabsContent value="cohere">
    ```bash
    npm i @langchain/cohere
    ```

    Add environment variables

    ```bash
    COHERE_API_KEY=your-api-key
    ```

    Instantiate the model

    ```jsx {0} showLineNumbers
    import { CohereEmbeddings } from "@langchain/cohere";

    const embeddings = new CohereEmbeddings({
    model: "embed-english-v3.0"
    });
    ````

  </TabsContent>
</Tabs>

And a vector store:

#### Pick your vector store:

<Tabs defaultValue="memory" className="pt-5 pb-1">
  <TabsList className="">
    <TabsTrigger value="memory">Memory</TabsTrigger>
    <TabsTrigger value="chroma">Chroma</TabsTrigger>
    <TabsTrigger value="faiss">Faiss</TabsTrigger>
    <TabsTrigger value="mongodb">MongoDB</TabsTrigger>
    <TabsTrigger value="pgvector">PGVector</TabsTrigger>
    <TabsTrigger value="pinecone">Pinecone</TabsTrigger>
    <TabsTrigger value="qdrant">Qdrant</TabsTrigger>
  </TabsList>

  <TabsContent value="memory">
    ```bash
    npm i langchain
    ```

    ```jsx {0} showLineNumbers
    import { MemoryVectorStore } from "langchain/vectorstores/memory";

    const vectorStore = new MemoryVectorStore(embeddings);
    ````

  </TabsContent>
  
  <TabsContent value="chroma">
    ```bash
    npm i @langchain/community
    ```

    ```jsx {0} showLineNumbers
    import { Chroma } from "@langchain/community/vectorstores/chroma";

    const vectorStore = new Chroma(embeddings, {
    collectionName: "a-test-collection",
    });
    ````

  </TabsContent>

  <TabsContent value="faiss">
    ```bash
    npm i @langchain/community
    ```

    ```jsx {0} showLineNumbers
    import { FaissStore } from "@langchain/community/vectorstores/faiss";

    const vectorStore = new FaissStore(embeddings, {});
    ````

  </TabsContent>

  <TabsContent value="mongodb">
    ```bash
    npm i @langchain/mongodb
    ```

    ```jsx {0} showLineNumbers
    import { MongoDBAtlasVectorSearch } from "@langchain/mongodb"
    import { MongoClient } from "mongodb";

    const client = new MongoClient(process.env.MONGODB_ATLAS_URI || "");
    const collection = client
    .db(process.env.MONGODB_ATLAS_DB_NAME)
    .collection(process.env.MONGODB_ATLAS_COLLECTION_NAME);

    const vectorStore = new MongoDBAtlasVectorSearch(embeddings, {
    collection: collection,
    indexName: "vector_index",
    textKey: "text",
    embeddingKey: "embedding",
    });
    ````

  </TabsContent>

  <TabsContent value="pgvector">
    ```bash
    npm i @langchain/community
    ```

    ```jsx {0} showLineNumbers
    import PGVectorStore from "@langchain/community/vectorstores/pgvector";

    const vectorStore = await PGVectorStore.initialize(embeddings, {})
    ````

  </TabsContent>

  <TabsContent value="pinecone">
    ```bash
    npm i @langchain/pinecone
    ```

    ```jsx {0} showLineNumbers
    import { PineconeStore } from "@langchain/pinecone";
    import { Pinecone as PineconeClient } from "@pinecone-database/pinecone";

    const pinecone = new PineconeClient();
    const vectorStore = new PineconeStore(embeddings, {
        pineconeIndex,
        maxConcurrency: 5,
    });
    ````

  </TabsContent>

  <TabsContent value="qdrant">
    ```bash
    npm i @langchain/qdrant
    ```

    ```jsx {0} showLineNumbers
    import { QdrantVectorStore } from "@langchain/qdrant";

    const vectorStore = await QdrantVectorStore.fromExistingCollection(embeddings, {
    url: process.env.QDRANT_URL,
    collectionName: "langchainjs-testing",
    });
    ````

  </TabsContent>
</Tabs>

### Dependencies

In addition, we’ll use the following packages:

<Tabs defaultValue="npm" className="pt-5 pb-1">
    <TabsList className="">
    <TabsTrigger value="npm">npm</TabsTrigger>
    <TabsTrigger value="yarn">yarn</TabsTrigger>
    <TabsTrigger value="pnpm">pnpm</TabsTrigger>
    </TabsList>

    <TabsContent value="npm">
    ```bash
    npm i langchain @langchain/community @langchain/langgraph cheerio
    ```
    </TabsContent>

    <TabsContent value="yarn">
    ```bash
    yarn add langchain @langchain/community @langchain/langgraph cheerio
    ```
    </TabsContent>

    <TabsContent value="pnpm">
    ```bash
    pnpm add langchain @langchain/community @langchain/langgraph cheerio
    ```
    </TabsContent>

</Tabs>

### LangSmith

Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.

Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:

```bash
export LANGCHAIN_TRACING_V2=true
export LANGCHAIN_API_KEY=YOUR_KEY

# Reduce tracing latency if you are not in a serverless environment
# export LANGCHAIN_CALLBACKS_BACKGROUND=true
```
