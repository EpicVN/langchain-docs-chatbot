---
title: Introduction
description: Here you'll find answers to “How do I….?” types of questions. These guides are goal-oriented and concrete; they're meant to help you complete a specific task. For conceptual explanations see Conceptual Guides. For end-to-end walkthroughs see Tutorials. For comprehensive descriptions of every class and function see API Reference.
keywords: ["how-to-guides", "guide", "nextjs", "documents"]
---

## Key features

This highlights functionality that is core to using LangChain.

    - How to: return structured data from an LLM
    - How to: use a chat model to call tools
    - How to: stream runnables
    - How to: debug your LLM apps

## LangChain Expression Language (LCEL)

LangChain Expression Language is a way to create arbitrary custom chains. It is built on the Runnable protocol.

LCEL cheatsheet: For a quick overview of how to use the main LCEL primitives.

    - How to: chain runnables
    - How to: stream runnables
    - How to: invoke runnables in parallel
    - How to: attach runtime arguments to a runnable
    - How to: run custom functions
    - How to: pass through arguments from one step to the next
    - How to: add values to a chain's state
    - How to: add message history
    - How to: route execution within a chain
    - How to: add fallbacks
    - How to: cancel execution

## Components

These are the core building blocks you can use when building applications.

### Prompt templates
Prompt Templates are responsible for formatting user input into a format that can be passed to a language model.

    - How to: use few shot examples
    - How to: use few shot examples in chat models
    - How to: partially format prompt templates
    - How to: compose prompts together

### Example selectors
Example Selectors are responsible for selecting the correct few shot examples to pass to the prompt.

    - How to: use example selectors
    - How to: select examples by length
    - How to: select examples by semantic similarity
    - How to: select examples from LangSmith few-shot datasets

### Chat models
Chat Models are newer forms of language models that take messages in and output a message.

    - How to: do function/tool calling
    - How to: get models to return structured output
    - How to: cache model responses
    - How to: create a custom chat model class
    - How to: get log probabilities
    - How to: stream a response back
    - How to: track token usage
    - How to: pass tool outputs to chat models
    - How to: stream tool calls
    - How to: few shot prompt tool behavior
    - How to: force a specific tool call
    - How to: disable parallel tool calling
    - How to: init any model in one line

### Messages
Messages are the input and output of chat models. They have some content and a role, which describes the source of the message.

    - How to: trim messages
    - How to: filter messages
    - How to: merge consecutive messages of the same type

### LLMs
What LangChain calls LLMs are older forms of language models that take a string in and output a string.

    - How to: cache model responses
    - How to: create a custom LLM class
    - How to: stream a response back
    - How to: track token usage