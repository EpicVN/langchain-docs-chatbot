[
  {
    "slug": "/how_to",
    "title": "Introduction",
    "description": "Here you'll find answers to “How do I….?” types of questions. These guides are goal-oriented and concrete; they're meant to help you complete a specific task. For conceptual explanations see Conceptual Guides. For end-to-end walkthroughs see Tutorials. For comprehensive descriptions of every class and function see API Reference.",
    "content": "## Key features\n\nThis highlights functionality that is core to using LangChain.\n\n* How to: return structured data from an LLM\n* How to: use a chat model to call tools\n* How to: stream runnables\n* How to: debug your LLM apps\n\n## LangChain Expression Language (LCEL)\n\nLangChain Expression Language is a way to create arbitrary custom chains. It is built on the Runnable protocol.\n\nLCEL cheatsheet: For a quick overview of how to use the main LCEL primitives.\n\n* How to: chain runnables\n* How to: stream runnables\n* How to: invoke runnables in parallel\n* How to: attach runtime arguments to a runnable\n* How to: run custom functions\n* How to: pass through arguments from one step to the next\n* How to: add values to a chain's state\n* How to: add message history\n* How to: route execution within a chain\n* How to: add fallbacks\n* How to: cancel execution\n\n## Components\n\nThese are the core building blocks you can use when building applications.\n\n### Prompt templates\n\nPrompt Templates are responsible for formatting user input into a format that can be passed to a language model.\n\n* How to: use few shot examples\n* How to: use few shot examples in chat models\n* How to: partially format prompt templates\n* How to: compose prompts together\n\n### Example selectors\n\nExample Selectors are responsible for selecting the correct few shot examples to pass to the prompt.\n\n* How to: use example selectors\n* How to: select examples by length\n* How to: select examples by semantic similarity\n* How to: select examples from LangSmith few-shot datasets\n\n### Chat models\n\nChat Models are newer forms of language models that take messages in and output a message.\n\n* How to: do function/tool calling\n* How to: get models to return structured output\n* How to: cache model responses\n* How to: create a custom chat model class\n* How to: get log probabilities\n* How to: stream a response back\n* How to: track token usage\n* How to: pass tool outputs to chat models\n* How to: stream tool calls\n* How to: few shot prompt tool behavior\n* How to: force a specific tool call\n* How to: disable parallel tool calling\n* How to: init any model in one line\n\n### Messages\n\nMessages are the input and output of chat models. They have some content and a role, which describes the source of the message.\n\n* How to: trim messages\n* How to: filter messages\n* How to: merge consecutive messages of the same type\n\n### LLMs\n\nWhat LangChain calls LLMs are older forms of language models that take a string in and output a string.\n\n* How to: cache model responses\n* How to: create a custom LLM class\n* How to: stream a response back\n* How to: track token usage\n"
  },
  {
    "slug": "/introduction/changelog",
    "title": "Changelog",
    "description": "Changelogs and improvements to the Documents projects.",
    "content": ""
  },
  {
    "slug": "/introduction",
    "title": "Introduction",
    "description": "This section provides an overview of how to get started with the Documents, Next.js Document Starter Kit.",
    "content": "![Banner](/images/banner.png \"Documents\")\n\n## LangChain\n\n**LangChain** is a framework for developing applications powered by large language models (LLMs).\n\n**LangChain** simplifies every stage of the LLM application lifecycle:\n\n* **Development**: Build your applications using LangChain's open-source components and third-party integrations. Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\n* **Productionization**: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\n* **Deployment**: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\n\n<CardGrid>\n  <Card subtitle=\"Instructions\" title=\"Installation\" description=\"Get started with LangChain using our quick start installation guide to get your project started.\" href=\"/docs/introduction/installation\" />\n\n  <Card subtitle=\"Tutorials\" title=\"Get started\" description=\"New to LangChain or LLM app development in general? Read this material to quickly get up and running building your first applications.\" href=\"/docs/tutorials\" />\n\n  <Card subtitle=\"Support\" title=\"Technology, Information and Internet\" description=\"We're on a mission to make it easy to build the LLM apps of tomorrow, today.\" href=\"https://github.com/langchain-ai\" external={true} />\n</CardGrid>\n\n## Why LangChain?\n\nThe goal of the langchain package and LangChain the company is to make it as easy possible for developers to build applications that reason.\r\nWhile LangChain originally started as a single open source package, it has evolved into a company and a whole ecosystem. This page will talk about the\r\nLangChain ecosystem as a whole. Most of the components within in the LangChain ecosystem can be used by themselves - so if you feel particularly drawn to\r\ncertain components but not others, that is totally fine! Pick and choose whichever components you like best.\n\n## Key Features\n\nThere are several primary needs that LangChain aims to address:\n\n| Feature                               | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\r\n| ------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\r\n| **Standardized component interfaces** | The growing number of models and related components for AI applications has resulted in a wide variety of different APIs that developers need to learn and use. This diversity can make it challenging for developers to switch between providers or combine components when building applications. LangChain exposes a standard interface for key components, making it easy to switch between providers.                                                                    |\r\n| **Orchestration**                     | As applications become more complex, combining multiple components and models, there's a growing need to efficiently connect these elements into control flows that can accomplish diverse tasks. Orchestration is crucial for building such applications.                                                                                                                                                                                                                    |\r\n| **Observability and evaluation**      | As applications become more complex, it becomes increasingly difficult to understand what is happening within them. Furthermore, the pace of development can become rate-limited by the paradox of choice: for example, developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. Observability and evaluations can help developers monitor their applications and rapidly answer these types of questions with confidence |\n\n## How to Use These Documents\n\nYou'll find the documentation's navigation bar on the left side of the screen. The pages are organized in a logical sequence, progressing from foundational\r\ntopics to more advanced concepts, allowing you to follow along step-by-step as you build your documentation. However, feel free to explore the content in\r\nany order, selecting the sections most relevant to your use case.\n\nOn the right side of the screen, a table of contents provides easy navigation between sections of each page. For quick access to specific content, you can\r\nuse the search bar at the top or the search shortcut (`Ctrl+K` or `Cmd+K`).\n\nHead over to the [Installation Guide](/docs/introduction/installation).\n\n## Join our Community\n\nIf you have questions about anything related to LangChain, you're always welcome to ask our community on\r\n[GitHub](https://github.com/langchain-ai/langchainjs) and [X (Twitter)](https://twitter.com/LangChainAI).\n"
  },
  {
    "slug": "/introduction/installation",
    "title": "Installation",
    "description": "This guide covers the installation of LangChain and how to use it in your project.",
    "content": "To install and use LangChain, you need to have several prerequisites in place. Here's a list of all the essential pre-requisites\r\nfor setting up and working on this project.\n\n## Prerequisites\n\n<Step>\n  <StepItem title=\"LangChain is written in TypeScript and can be used in:\">\n    * **Node.js** (ESM and CommonJS) - 18.x, 19.x, 20.x\n    * **Cloudflare Workers**\n    * **Vercel / Next.js** (Browser, Serverless and Edge functions)\n    * **Supabase Edge Functions**\n    * **Browser**\n    * **Deno**\n    * **Bun**\n\n    However, note that individual integrations may not be supported in all environments.\n  </StepItem>\n</Step>\n\n## Installation\n\n<Step>\n  <StepItem title=\"The main LangChain packages\">\n    To install the main langchain package, run:\n\n    ```bash\n    npm install langchain @langchain/core\n    ```\n\n    While this package acts as a sane starting point to using LangChain, much of the value of LangChain comes when integrating it with various model providers, datastores, etc. By default, the dependencies needed to do that are NOT installed. You will need to install the dependencies for specific integrations separately. We'll show how to do that in the next sections of this guide.\n\n    Please also see the section on installing integration packages for some special considerations when installing LangChain packages.\n  </StepItem>\n\n  <StepItem title=\"How to install ecosystem packages\">\n    With the exception of the langsmith SDK, all packages in the LangChain ecosystem depend on @langchain/core, which contains base classes and abstractions that other packages use. The dependency graph below shows how the difference packages are related. A directed arrow indicates that the source package depends on the target package:\n\n    <Note title=\"Note\" type=\"danger\">\n      It is important that your app only uses one version of @langchain/core. Common\r\n      package managers may introduce additional versions when resolving direct\r\n      dependencies, even if you don't intend this. See this section on installing\r\n      integration packages for more information and ways to remedy this.\n    </Note>\n\n     \n\n    <StepItem title=\"@langchain/community\">\n      The **@langchain/community** package contains a range of third-party integrations. Install with:\n\n      ```bash\n      npm install @langchain/community @langchain/core\n      ```\n\n      There are also more granular packages containing LangChain integrations for individual providers.\n    </StepItem>\n\n     \n\n    <StepItem title=\"@langchain/core\">\n      The **@langchain/core** package contains base abstractions that the rest of the LangChain ecosystem uses, along with the LangChain Expression Language. It should be installed separately:\n\n      ```bash\n      npm install @langchain/core\n      ```\n    </StepItem>\n\n     \n\n    <StepItem title=\"LangGraph\">\n      **LangGraph.js** is a library for building stateful, multi-actor applications with LLMs. It integrates smoothly with LangChain, but can be used without it.\n\n      Install with:\n\n      ```bash\n      npm install @langchain/langgraph @langchain/core\n      ```\n    </StepItem>\n\n     \n\n    <StepItem title=\"LangSmith SDK\">\n      The **LangSmith SDK** is automatically installed by LangChain. If you're not using it with LangChain, install with:\n\n      ```bash\n      npm install langsmith\n      ```\n    </StepItem>\n  </StepItem>\n\n  <StepItem title=\"Installing integration packages\">\n    To install the main langchain package, run:\n\n    ```bash\n    npm install langchain @langchain/core\n    ```\n\n    LangChain supports packages that contain module integrations with individual third-party providers. They can be as specific as @langchain/anthropic, which contains integrations just for Anthropic models, or as broad as @langchain/community, which contains broader variety of community contributed integrations.\n\n    These packages, as well as the main LangChain package, all have @langchain/core as a peer dependency to avoid package managers installing multiple versions of the same package. It contains the base abstractions that these integration packages extend.\n\n    To ensure that all integrations and their types interact with each other properly, it is important that they all use the same version of @langchain/core. If you encounter type errors around base classes, you may need to guarantee that your package manager is resolving a single version of @langchain/core. To do so, you can add a \"resolutions\" or \"overrides\" field like the following in your project's package.json. The name will depend on your package manager:\n\n    <Note title=\"TIP\" type=\"success\">\n      The **resolutions** or **pnpm.overrides** fields for **yarn** or **pnpm** must\r\n      be set in the root **package.json** file.\n    </Note>\n\n    If you are using **yarn**:\n\n    <Note title=\"yarn package.json\" type=\"note\">\n      ```bash\n      {\r\n        \"name\": \"your-project\",\r\n        \"version\": \"0.0.0\",\r\n        \"private\": true,\r\n        \"engines\": {\r\n          \"node\": \">=18\"\r\n        },\r\n        \"dependencies\": {\r\n          \"@langchain/anthropic\": \"^0.0.2\",\r\n          \"@langchain/core\": \"^0.3.0\",\r\n          \"langchain\": \"0.0.207\"\r\n        },\r\n        \"resolutions\": {\r\n          \"@langchain/core\": \"0.3.0\"\r\n        }\r\n      }\n      ```\n    </Note>\n\n    You can also try running the **yarn dedupe** command if you are on **yarn** version 2 or higher.\n\n    Or for **npm**:\n\n    <Note title=\"npm package.json\" type=\"note\">\n      ```bash\n      {\r\n        \"name\": \"your-project\",\r\n        \"version\": \"0.0.0\",\r\n        \"private\": true,\r\n        \"engines\": {\r\n          \"node\": \">=18\"\r\n        },\r\n        \"dependencies\": {\r\n          \"@langchain/anthropic\": \"^0.0.2\",\r\n          \"@langchain/core\": \"^0.3.0\",\r\n          \"langchain\": \"0.0.207\"\r\n        },\r\n        \"overrides\": {\r\n          \"@langchain/core\": \"0.3.0\"\r\n        }\r\n      }\n      ```\n    </Note>\n\n    You can also try the **npm dedupe** command.\n\n    Or for **pnpm**:\n\n    <Note title=\"pnpm package.json\" type=\"note\">\n      ```bash\n      {\r\n        \"name\": \"your-project\",\r\n        \"version\": \"0.0.0\",\r\n        \"private\": true,\r\n        \"engines\": {\r\n          \"node\": \">=18\"\r\n        },\r\n        \"dependencies\": {\r\n          \"@langchain/anthropic\": \"^0.0.2\",\r\n          \"@langchain/core\": \"^0.3.0\",\r\n          \"langchain\": \"0.0.207\"\r\n        },\r\n        \"pnpm\": {\r\n          \"overrides\": {\r\n            \"@langchain/core\": \"0.3.0\"\r\n          }\r\n        }\r\n      }\n      ```\n    </Note>\n\n    You can also try the **pnpm dedupe** command.\n  </StepItem>\n\n  <StepItem title=\"Loading the library\">\n    <StepItem title=\"TypeScript\">\n      LangChain is written in TypeScript and provides type definitions for all of its public APIs.\n    </StepItem>\n\n     \n\n    <StepItem title=\"ESM\">\n      LangChain provides an ESM build targeting Node.js environments. You can import it using the following syntax:\n\n      ```bash\n      npm install @langchain/openai @langchain/core\n      ```\n\n      <Note title=\"Import\" type=\"note\">\n        `bash import {ChatOpenAI} from \"@langchain/openai\"; `\n      </Note>\n\n      If you are using TypeScript in an ESM project we suggest updating your tsconfig.json to include the following:\n\n      <Note title=\"tsconfig.json\" type=\"note\">\n        ```bash\n        {\r\n          \"compilerOptions\": {\r\n            ...\r\n            \"target\": \"ES2020\", // or higher\r\n            \"module\": \"nodenext\",\r\n          }\r\n        }\n        ```\n      </Note>\n    </StepItem>\n\n     \n\n    <StepItem title=\"CommonJS\">\n      LangChain provides a CommonJS build targeting Node.js environments. You can import it using the following syntax:\n\n      <Note title=\"Import\" type=\"note\">\n        ```bash\n        const {ChatOpenAI} = require(\"@langchain/openai\"); \n        ```\n      </Note>\n    </StepItem>\n\n     \n\n    <StepItem title=\"Cloudflare Workers\">\n      LangChain can be used in Cloudflare Workers. You can import it using the following syntax:\n\n      <Note title=\"Import\" type=\"note\">\n        ```bash\n        import { ChatOpenAI } from \"@langchain/openai\";\n        ```\n      </Note>\n    </StepItem>\n\n     \n\n    <StepItem title=\"Vercel / Next.js\">\n      LangChain can be used in Vercel / Next.js. We support using LangChain in frontend components, in Serverless functions and in Edge functions. You can import it using the following syntax:\n\n      <Note title=\"Import\" type=\"note\">\n        ```bash\n        import { ChatOpenAI } from \"@langchain/openai\";\n        ```\n      </Note>\n    </StepItem>\n\n     \n\n    <StepItem title=\"Deno / Supabase Edge Functions\">\n      LangChain can be used in Deno / Supabase Edge Functions. You can import it using the following syntax:\n\n      <Note title=\"Import\" type=\"note\">\n        ```bash\n        import { ChatOpenAI } from \"https://esm.sh/@langchain/openai\";\n        ```\n      </Note>\n\n      or\n\n      <Note title=\"Import\" type=\"note\">\n        ```bash\n        import { ChatOpenAI } from \"npm:@langchain/openai\";\n        ```\n      </Note>\n    </StepItem>\n\n     \n\n    <StepItem title=\"Browser\">\n      LangChain can be used in the browser. In our CI we test bundling LangChain with Webpack and Vite, but other bundlers should work too. You can import it using the following syntax:\n\n      <Note title=\"Import\" type=\"note\">\n        ```bash\n        import { ChatOpenAI } from \"@langchain/openai\";\n        ```\n      </Note>\n    </StepItem>\n\n     \n\n    <StepItem title=\"Unsupported: Node.js 16\">\n      We do not support Node.js 16, but if you still want to run LangChain on Node.js 16, you will need to follow the instructions in this section. We do not guarantee that these instructions will continue to work in the future.\n\n      You will have to make fetch available globally, either:\n\n      * run your application with **NODE\\_OPTIONS='--experimental-fetch' node ...**, or\n      * install **node-fetch** and follow the instructions **here**\r\n        You'll also need to polyfill **ReadableStream** by installing:\n\n      ```bash\n        npm i web-streams-polyfill@4\n      ```\n\n      And then adding it to the global namespace in your main entrypoint:\n\n      <Note title=\"Import\" type=\"note\">\n        ```bash\n        import \"web-streams-polyfill/polyfill\";\n        ```\n      </Note>\n\n      Additionally you'll have to polyfill **structuredClone**, eg. by installing **core-js** and following the instructions **here**.\n\n      If you are running Node.js 18+, you do not need to do anything.\n    </StepItem>\n  </StepItem>\n</Step>\n\n## Was this page helpful?\n\nYou can also leave detailed feedback on [GitHub](https://github.com/langchain-ai/langchainjs).\n"
  },
  {
    "slug": "/markup/cards",
    "title": "Cards",
    "description": "This section provides an overview of Introduction.",
    "content": "Lorem ipsum dolor sit amet consectetur adipisicing elit. Numquam iste dolorum tempore consectetur explicabo tempora provident quia maxime, molestiae, facilis aperiam et, error illum vel ullam? Quis architecto dolore ullam\n\n## Small Card\n\n<CardGrid>\n  <Card title=\"Page Structure\" href=\"/docs/deep/deeper\" icon=\"alignJustify\" variant=\"small\" description=\"test description\" />\n\n  <Card title=\"Page Structure\" href=\"/docs/deep/deeper\" icon=\"alignJustify\" variant=\"small\" />\n\n  <Card title=\"Rubix Studios\" href=\"https://www.rubixstudios.com.au\" icon=\"alignJustify\" external={true} variant=\"small\" />\n</CardGrid>\n\n## Large Card\n\n<CardGrid>\n  <Card subtitle=\"Instructions\" title=\"Installation\" description=\"Get started with Documents using our quick start installation guide to get your project started.\" href=\"/docs/introduction/installation\" />\n\n  <Card subtitle=\"Setup\" title=\"Site Settings\" description=\"Setting up your Documents projects layout, links and search engine optimisation.\" href=\"/docs/introduction/setup\" />\n\n  <Card subtitle=\"Support\" title=\"Rubix Studios\" description=\"Australia's leading branding, marketing and web development company.\" href=\"https://www.rubixstudios.com.au/\" external={true} />\n</CardGrid>\n\n## Image Card\n\n<CardGrid>\n  <Card title=\"Rubix Studios\" href=\"https://www.rubixstudios.com.au\" image=\"/images/og-image.png\" external={true} variant=\"image\" />\n\n  <Card title=\"Rubix Studios\" href=\"https://www.rubixstudios.com.au\" image=\"/images/og-image.png\" variant=\"image\" />\n\n  <Card title=\"Rubix Studios\" href=\"https://www.rubixstudios.com.au\" image=\"/images/og-image.png\" variant=\"image\" />\n</CardGrid>\n"
  },
  {
    "slug": "/markup/diagrams",
    "title": "Diagrams",
    "description": "This section provides an overview of Introduction.",
    "content": "Lorem ipsum dolor sit amet consectetur adipisicing elit. Numquam iste dolorum tempore consectetur explicabo tempora provident quia maxime, molestiae, facilis aperiam et, error illum vel ullam? Quis architecto dolore ullam\n\n## Flowchart\n\n## Decision Tree\n\n## Entity-Relationship Diagram\n"
  },
  {
    "slug": "/markup/filetree",
    "title": "Filetree",
    "description": "This section provides an overview of Introduction.",
    "content": "Lorem ipsum dolor sit amet consectetur adipisicing elit. Numquam iste dolorum tempore consectetur explicabo tempora provident quia maxime, molestiae, facilis aperiam et, error illum vel ullam? Quis architecto dolore ullam\n\n<FileTree>\n  <Folder name=\"src\" label=\"Source Code\">\n    <File name=\"index.tsx\" label=\"index.tsx\" />\n\n    <Folder name=\"components\" label=\"Components\">\n      <File name=\"button.tsx\" label=\"Button Component\" />\n\n      <File name=\"input.tsx\" label=\"Input Component\" />\n    </Folder>\n\n    <Folder name=\"pages\" label=\"Pages\">\n      <File name=\"home.tsx\" label=\"Home Page\" />\n\n      <File name=\"about.tsx\" label=\"About Page\" />\n    </Folder>\n  </Folder>\n</FileTree>\n"
  },
  {
    "slug": "/markup",
    "title": "Introduction",
    "description": "This section provides an overview of Introduction.",
    "content": "Lorem ipsum dolor sit amet consectetur adipisicing elit. Numquam iste dolorum tempore consectetur explicabo tempora provident quia maxime, molestiae, facilis aperiam et, error illum vel ullam? Quis architecto dolore ullam\n\n* \\[x] Write the press release\n* \\[ ] Update the website\n* \\[ ] Contact the media\n\n| Syntax        | Description |   Test Text |\r\n| :------------ | :---------: | ----------: |\r\n| Header        |    Title    | Here's this |\r\n| Paragraph     |    Text     |    And more |\r\n| Strikethrough |             |    ~~Text~~ |\n\n# Sample Document with Mermaid\n\nHere is a Mermaid diagram:\n\nThis diagram should render automatically without any extra imports.\n\n## Getting Started\n\nTo begin using the Documentation Template, follow these simple steps:\n\n* Start by cloning the repository to your local machine.\n\nLorem ipsum dolor sit amet consectetur adipisicing elit. Reprehenderit quae iure nulla deserunt dolore quam pariatur minus sapiente accusantium. Optio, necessitatibus sequi. Veritatis, aspernatur? Possimus quis repellat eum vitae eveniet.\n\n## Blockquotes\n\nBlockquotes are useful for emphasizing key points or quoting external sources:\n\n> \"Documentation is a love letter that you write to your future self.\" - Damian Conway\n\nFeel free to use blockquotes to highlight important information or quotes relevant to your documentation.\n\n## Code Examples with switch\n\nHere a custom tab component from shadcn ui is used.\n\n## Conclusion\n\nThank you for choosing the Documentation Template for your project. Whether you're documenting software, APIs, or processes, we're here to support you in creating clear and effective documentation. Happy documenting!\n"
  },
  {
    "slug": "/markup/lists",
    "title": "Lists",
    "description": "This section provides an overview of Introduction.",
    "content": "Lorem ipsum dolor sit amet consectetur adipisicing elit. Numquam iste dolorum tempore consectetur explicabo tempora provident quia maxime, molestiae, facilis aperiam et, error illum vel ullam? Quis architecto dolore ullam\n\n## Checklist\n\n* \\[x] Write the press release\n* \\[ ] Update the website\n* \\[ ] Contact the media\n\n## Simple List\n\n* Item 1\n* Item 2\n* Item 3\n\n## Number List\n\n1. Research\n2. Draft the content\n3. Review and edit\n4. Publish\n\n## Nested List\n\n* Main Category 1\n  * Sub Item 1.1\n  * Sub Item 1.2\n* Main Category 2\n  * Sub Item 2.1\n  * Sub Item 2.2\n"
  },
  {
    "slug": "/markup/maths",
    "title": "Maths",
    "description": "This section provides an overview of Introduction.",
    "content": "Lorem ipsum dolor sit amet consectetur adipisicing elit. Numquam iste dolorum tempore consectetur explicabo tempora provident quia maxime, molestiae, facilis aperiam et, error illum vel ullam? Quis architecto dolore ullam\n\n## Basic Algebra\n\nThe area of a circle ($$A$$) can be calculated using the radius ($$r$$) as follows:\n\n```math\nA = \\pi r^2\n```\n\n## Quadratic Formula\n\nThe quadratic formula for solving an equation of the form $$ax^2 + bx + c = 0$$ is:\n\n```math\nx = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\n```\n\n## Newton's Second Law of Motion\n\nNewton's second law of motion states that force ($$F$$) is the product of mass ($$m$$) and acceleration ($$a$$):\n\n```math\nF = ma\n```\n\n## Pythagorean Theorem\n\nThe Pythagorean theorem relates the lengths of the sides of a right triangle:\n\n```math\na^2 + b^2 = c^2\n```\n\n## Einstein's Mass-Energy Equivalence\n\nEinstein's famous equation relates energy ($$E$$), mass ($$m$$), and the speed of light ($$c$$):\n\n```math\nE = mc^2\n```\n"
  },
  {
    "slug": "/markup/notes",
    "title": "Notes",
    "description": "This section provides an overview of Introduction.",
    "content": "Lorem ipsum dolor sit amet consectetur adipisicing elit. Numquam iste dolorum tempore consectetur explicabo tempora provident quia maxime, molestiae, facilis aperiam et, error illum vel ullam? Quis architecto dolore ullam\n\n## Standard Note\n\n<Note title=\"Required\">\n  The project's search functionality relies on the Husky's automation to build `search-data/documents.json` ensure git commit is performed to generate this file.\n</Note>\n\n## Success Note\n\n<Note title=\"Required\" type=\"success\">\n  The project's search functionality relies on the Husky's automation to build `search-data/documents.json` ensure git commit is performed to generate this file.\n</Note>\n\n## Warning Note\n\n<Note title=\"Required\" type=\"warning\">\n  The project's search functionality relies on the Husky's automation to build `search-data/documents.json` ensure git commit is performed to generate this file.\n</Note>\n\n## Danger Note\n\n<Note title=\"Required\" type=\"danger\">\n  The project's search functionality relies on the Husky's automation to build `search-data/documents.json` ensure git commit is performed to generate this file.\n</Note>\n"
  },
  {
    "slug": "/markup/steps",
    "title": "Steps",
    "description": "This section provides an overview of Introduction.",
    "content": "The `<Step>` and `<StepItem>` components allow you to create structured step-by-step guides in your documentation. These components are particularly useful when you want to break down a process or tutorial into easy-to-follow stages.\n\n## Steps\n\nTo create a step-by-step guide in your MDX, you can use the following structure:\n\n<Step>\n  <StepItem title=\"Install Node.js\">\n    Make sure you have Node.js installed on your machine. You can download it from [here](https://nodejs.org).\n\n    To verify the installation, run the following command:\n\n    ```bash\n    node -v\n    ```\n  </StepItem>\n\n  <StepItem title=\"Install Dependencies\">\n    After cloning the repository, navigate to the project directory and install the necessary dependencies:\n\n    ```bash\n    npm install\n    ```\n  </StepItem>\n\n  <StepItem title=\"Run the Project\">\n    Start the development server:\n\n    ```bash\n    npm run dev\n    ```\n\n    You can access the application at:\n\n    ```bash\n    http://localhost:3000\n    ```\n  </StepItem>\n</Step>\n"
  },
  {
    "slug": "/markup/table",
    "title": "Table",
    "description": "This section provides an overview of Introduction.",
    "content": "Lorem ipsum dolor sit amet consectetur adipisicing elit. Numquam iste dolorum tempore consectetur explicabo tempora provident quia maxime, molestiae, facilis aperiam et, error illum vel ullam? Quis architecto dolore ullam\n\n| Syntax        | Description |   Test Text |\r\n| :------------ | :---------: | ----------: |\r\n| Header        |    Title    | Here's this |\r\n| Paragraph     |    Text     |    And more |\r\n| Strikethrough |             |    ~~Text~~ |\n\n| Feature     | Documentation Link | Notes                  |\r\n| :---------- | :----------------: | ----------------------: |\r\n| **Feature A**| [Docs](#)           | For more info click here |\r\n| **Feature B**| [Guide](#)          | See the full guide here  |\r\n| **Feature C**| [Setup](#)          | Setup instructions       |\n"
  },
  {
    "slug": "/markup/tabs",
    "title": "Tabs",
    "description": "This section provides an overview of Introduction.",
    "content": "Lorem ipsum dolor sit amet consectetur adipisicing elit. Numquam iste dolorum tempore consectetur explicabo tempora provident quia maxime, molestiae, facilis aperiam et, error illum vel ullam? Quis architecto dolore ullam\n\n## Code Examples with switch\n\nHere a custom tab component from shadcn ui is used.\n"
  },
  {
    "slug": "/orchestration/chatbot",
    "title": "Build a Chatbot",
    "description": "This tutorial will familiarize you with LangChain’s document loader, embedding, and vector store abstractions. These abstractions are designed to support retrieval of data– from (vector) databases and other sources– for integration with LLM workflows. They are important for applications that fetch data to be reasoned over as part of model inference, as in the case of retrieval-augmented generation, or RAG (see our RAG tutorial here).",
    "content": "<Note title=\"Prerequisites\" type=\"success\">\n  This guide assumes familiarity with the following concepts:\n\n  * Chat Models\n  * Prompt Templates\n  * Chat History\r\n    This guide requires **langgraph >= 0.2.28**.\n</Note>\n\n<Note title=\"Note\" type=\"note\">\n  This tutorial previously built a chatbot using **RunnableWithMessageHistory**. You can access this version of the tutorial in the v0.2 docs.\n\n  The LangGraph implementation offers a number of advantages over **RunnableWithMessageHistory**, including the ability to persist arbitrary components of an application's state (instead of only messages).\n</Note>\n\n## Overview\n\nWe’ll go over an example of how to design and implement an LLM-powered chatbot. This chatbot will be able to have a conversation and remember previous interactions.\n\nNote that this chatbot that we build will only use the language model to have a conversation. There are several other related concepts that you may be looking for:\n\n* [Conversational RAG](/docs/tutorials/qa_chat_history): Enable a chatbot experience over an external source of data\n* [Agents](https://langchain-ai.github.io/langgraphjs/tutorials/multi_agent/agent_supervisor/): Build a chatbot that can take actions\n\nThis tutorial will cover the basics which will be helpful for those two more advanced topics, but feel free to skip directly to there should you choose.\n\n## Setup\n\n### Jupyter Notebook\n\nThis guide (and most of the other guides in the documentation) uses Jupyter notebooks and assumes the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them.\n\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebook. See here for instructions on how to install.\n\n### Installation\n\nFor this tutorial we will need **@langchain/core** and **langgraph**:\n\nFor more details, see our [Installation guide](/docs/introduction/installation).\n\n### LangSmith\n\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\n\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\n\n```bash\nprocess.env.LANGCHAIN_TRACING_V2 = \"true\";\r\nprocess.env.LANGCHAIN_API_KEY = \"...\";\n```\n\n## Quickstart\n\nFirst up, let’s learn how to use a language model by itself. LangChain supports many different language models that you can use interchangeably - select the one you want to use below!\n\n### Pick your chat model:\n\nInstall dependencies:\n\n<Note title=\"TIP\" type=\"success\">\n  See [this section for general instructions on installing integration\r\n  packages](/docs/introduction/installation#installation).\n</Note>\n"
  },
  {
    "slug": "/orchestration/graph",
    "title": "Build a Question Answering application over a Graph Database",
    "description": "In this guide we’ll go over the basic ways to create a Q&A chain over a graph database. These systems will allow us to ask a question about the data in a graph database and get back a natural language answer.",
    "content": "## Security note\n\nBuilding Q\\&A systems of SQL databases requires executing model-generated SQL queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your chain/agent’s needs. This will mitigate though not eliminate the risks of building a model-driven system. For more on general security best practices, [see here](/docs/security).\n\n## Architecture\n\nAt a high-level, the steps of these systems are:\n\n1. Convert question to SQL query: Model converts user input to a SQL query.\n2. Execute SQL query: Execute the query.\n3. Answer the question: Model responds to user input using the query results.\n\n![graph](/images/graph.png \"graph\")\n\n## Setup\n\nInstall dependencies:\n\n<Note title=\"TIP\" type=\"success\">\n  See [this section for general instructions on installing integration\r\n  packages](/docs/introduction/installation#installation).\n</Note>\n\nSet environment variables\r\nWe’ll use OpenAI in this example:\n\n```bash\nOPENAI_API_KEY=your-api-key\r\n\r\n# Optional, use LangSmith for best-in-class observability\r\nLANGSMITH_API_KEY=your-api-key\r\nLANGCHAIN_TRACING_V2=true\r\n\r\n# Reduce tracing latency if you are not in a serverless environment\r\n# LANGCHAIN_CALLBACKS_BACKGROUND=true\n```\n\nNext, we need to define Neo4j credentials. Follow [these installation](https://neo4j.com/docs/operations-manual/current/installation/) steps to set up a Neo4j database.\n\n```bash\nNEO4J_URI=\"bolt://localhost:7687\"\r\nNEO4J_USERNAME=\"neo4j\"\r\nNEO4J_PASSWORD=\"password\"\n```\n\nThe below example will create a connection with a Neo4j database and will populate it with example data about movies and their actors.\n\n```jsx {0} showLineNumbers\nimport \"neo4j-driver\";\r\nimport { Neo4jGraph } from \"@langchain/community/graphs/neo4j_graph\";\r\n\r\nconst url = process.env.NEO4J_URI;\r\nconst username = process.env.NEO4J_USER;\r\nconst password = process.env.NEO4J_PASSWORD;\r\nconst graph = await Neo4jGraph.initialize({ url, username, password });\r\n\r\n// Import movie information\r\nconst moviesQuery = `LOAD CSV WITH HEADERS FROM \r\n'https://raw.githubusercontent.com/tomasonjo/blog-datasets/main/movies/movies_small.csv'\r\nAS row\r\nMERGE (m:Movie {id:row.movieId})\r\nSET m.released = date(row.released),\r\n    m.title = row.title,\r\n    m.imdbRating = toFloat(row.imdbRating)\r\nFOREACH (director in split(row.director, '|') | \r\n    MERGE (p:Person {name:trim(director)})\r\n    MERGE (p)-[:DIRECTED]->(m))\r\nFOREACH (actor in split(row.actors, '|') | \r\n    MERGE (p:Person {name:trim(actor)})\r\n    MERGE (p)-[:ACTED_IN]->(m))\r\nFOREACH (genre in split(row.genres, '|') | \r\n    MERGE (g:Genre {name:trim(genre)})\r\n    MERGE (m)-[:IN_GENRE]->(g))`;\r\n\r\nawait graph.query(moviesQuery);\n```\n\n```bash\nSchema refreshed successfully.\n```\n\n```bash\n[]\n```\n\n## Graph schema\n\nIn order for an LLM to be able to generate a Cypher statement, it needs information about the graph schema. When you instantiate a graph object, it retrieves the information about the graph schema. If you later make any changes to the graph, you can run the **refreshSchema** method to refresh the schema information.\n\n```jsx {0} showLineNumbers\nawait graph.refreshSchema();\r\nconsole.log(graph.getSchema());\n```\n\n```bash\nNode properties are the following:\r\nMovie {imdbRating: FLOAT, id: STRING, released: DATE, title: STRING}, Person {name: STRING}, Genre {name: STRING}\r\nRelationship properties are the following:\r\n\r\nThe relationships are the following:\r\n(:Movie)-[:IN_GENRE]->(:Genre), (:Person)-[:DIRECTED]->(:Movie), (:Person)-[:ACTED_IN]->(:Movie)\n```\n\n## Chain\n\nLet’s use a simple chain that takes a question, turns it into a Cypher query, executes the query, and uses the result to answer the original question.\n\n![graph\\_1](/images/graph_1.png \"graph_1\")\n\nLangChain comes with a built-in chain for this workflow that is designed to work with Neo4j: **GraphCypherQAChain**.\n\n<Note title=\"Danger\" type=\"danger\">\n  The **GraphCypherQAChain** used in this guide will execute Cypher statements against the provided database. For production, make sure that the database connection uses credentials that are narrowly-scoped to only include necessary permissions.\n\n  Failure to do so may result in data corruption or loss, since the calling code may attempt commands that would result in deletion, mutation of data if appropriately prompted or reading sensitive data if such data is present in the database.\n</Note>\n\n```jsx {0} showLineNumbers\nimport { GraphCypherQAChain } from \"langchain/chains/graph_qa/cypher\";\r\nimport { ChatOpenAI } from \"@langchain/openai\";\r\n\r\nconst llm = new ChatOpenAI({ model: \"gpt-3.5-turbo\", temperature: 0 });\r\nconst chain = GraphCypherQAChain.fromLLM({\r\n  llm,\r\n  graph,\r\n});\r\nconst response = await chain.invoke({\r\n  query: \"What was the cast of the Casino?\",\r\n});\r\nconsole.log(response);\n```\n\n```bash\n{ result: \"James Woods, Joe Pesci, Robert De Niro, Sharon Stone\" }\n```\n\n### Next steps\n\nFor more complex query-generation, we may want to create few-shot prompts or add query-checking steps. For advanced techniques like this and more check out:\n\n* Prompting strategies: Advanced prompt engineering techniques.\n* Mapping values: Techniques for mapping values from questions to database.\n* Semantic layer: Techniques for working implementing semantic layers.\n* Constructing graphs: Techniques for constructing knowledge graphs.\n"
  },
  {
    "slug": "/orchestration",
    "title": "Orchestration",
    "description": "",
    "content": ""
  },
  {
    "slug": "/orchestration/qa_chat_history",
    "title": "Build a Retrieval Augmented Generation (RAG) App Part 2",
    "description": "In many Q&A applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of “memory” of past questions and answers, and some logic for incorporating those into its current thinking.",
    "content": "This is a the second part of a multi-part tutorial:\n\n* [Part 1](/docs/tutorials/rag) introduces RAG and walks through a minimal implementation.\n* Part 2 (this guide) extends the implementation to accommodate conversation-style interactions and multi-step retrieval processes.\n\nHere we focus on adding logic for incorporating historical messages. This involves the management of a chat history.\n\nWe will cover two approaches:\n\n1. [Chains](/#agents), in which we execute at most one retrieval step;\n2. [Agents](/#agents), in which we give an LLM discretion to execute multiple retrieval steps.\n\n<Note title=\"NOTE\" type=\"note\">\n  The methods presented here leverage tool-calling capabilities in modern chat\r\n  models. See this page for a table of models supporting tool calling features.\n</Note>\n\nFor the external knowledge source, we will use the same [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post by Lilian Weng from the [Part 1](/docs/tutorials/rag) of the RAG tutorial.\n\n## Setup\n\n### Components\n\nWe will need to select three components from LangChain’s suite of integrations.\n\nA chat model:\n\n#### Pick your chat model:\n\nInstall dependencies:\n\n<Note title=\"TIP\" type=\"success\">\n  See [this section for general instructions on installing integration\r\n  packages](/docs/introduction/installation#installation).\n</Note>\n\nAn embedding model:\n\n#### Pick your embedding model:\n\nInstall dependencies\n\nAnd a vector store:\n\n#### Pick your vector store:\n\n### Dependencies\n\nIn addition, we’ll use the following packages:\n\n### LangSmith\n\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\n\nNote that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:\n\n```bash\nexport LANGCHAIN_TRACING_V2=true\r\nexport LANGCHAIN_API_KEY=YOUR_KEY\r\n\r\n# Reduce tracing latency if you are not in a serverless environment\r\n# export LANGCHAIN_CALLBACKS_BACKGROUND=true\n```\n"
  },
  {
    "slug": "/orchestration/rag",
    "title": "Build a Retrieval Augmented Generation (RAG) App Part 1",
    "description": "One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.",
    "content": "This is a multi-part tutorial:\n\n* Part 1 (this guide) introduces RAG and walks through a minimal implementation.\n* [Part 2](/docs/tutorials/qa_chat_history) extends the implementation to accommodate conversation-style interactions and multi-step retrieval processes.\r\n  This tutorial will show how to build a simple Q\\&A application over a text data source. Along the way we’ll go over a typical Q\\&A architecture and highlight additional resources for more advanced Q\\&A techniques. We’ll also see how LangSmith can help us trace and understand our application. LangSmith will become increasingly helpful as our application grows in complexity.\n\nIf you’re already familiar with basic retrieval, you might also be interested in this high-level overview of different retrieval techinques.\n\nNote: Here we focus on Q\\&A for unstructured data. If you are interested for RAG over structured data, check out our tutorial on doing [question/answering over SQL data](/docs/tutorials/sql_qa).\n\n## Overview\n\nA typical RAG application has two main components:\n\n**Indexing**: a pipeline for ingesting data from a source and indexing it. This usually happens offline.\n\n**Retrieval and generation**: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n\nNote: the indexing portion of this tutorial will largely follow the [semantic search tutorial](/docs/tutorials/retrievers).\n\nThe most common full sequence from raw data to answer looks like:\n\n### Indexing\n\n1. Load: First we need to load our data. This is done with Document Loaders.\n2. Split: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won’t fit in a model’s finite context window.\n3. Store: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.\n\n![RAG](/images/rag.png \"RAG\")\n\n### Retrieval and generation\n\n1. Retrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\n2. Generate: A ChatModel / LLM produces an answer using a prompt that includes both the question with the retrieved data\n\n![RAG\\_1](/images/rag_1.png \"RAG_1\")\n\nOnce we’ve indexed our data, we will use LangGraph as our orchestration framework to implement the retrieval and generation steps.\n\n## Setup\n\n### Jupyter Notebook\n\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebooks. Going through guides in an interactive environment is a great way to better understand them. See here for instructions on how to install.\n\n### Installation\n\nThis guide requires **@langchain/community** and **pdf-parse**:\n\nFor more details, see our [Installation guide](/docs/introduction/installation).\n\n### LangSmith\n\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\n\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\n\n```bash\nexport LANGCHAIN_TRACING_V2=\"true\"\r\nexport LANGCHAIN_API_KEY=\"...\"\r\n\r\n# Reduce tracing latency if you are not in a serverless environment\r\n# export LANGCHAIN_CALLBACKS_BACKGROUND=true\n```\n\n## Components\n\nWe will need to select three components from LangChain’s suite of integrations.\n\nA chat model:\n\n### Pick your chat model:\n\nInstall dependencies:\n\n<Note title=\"TIP\" type=\"success\">\n  See [this section for general instructions on installing integration\r\n  packages](/docs/introduction/installation#installation).\n</Note>\n\nAn embedding model:\n\n### Pick your embedding model:\n\nInstall dependencies\n\n## Preview\n\nIn this guide we’ll build an app that answers questions about the website’s content. The specific website we will use is the [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\n\nWe can create a simple indexing pipeline and RAG chain to do this in ~50 lines of code.\n\n```jsx {0} showLineNumbers\nimport \"cheerio\";\r\nimport { CheerioWebBaseLoader } from \"@langchain/community/document_loaders/web/cheerio\";\r\nimport { Document } from \"@langchain/core/documents\";\r\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\r\nimport { pull } from \"langchain/hub\";\r\nimport { Annotation, StateGraph } from \"@langchain/langgraph\";\r\nimport { RecursiveCharacterTextSplitter } from \"@langchain/textsplitters\";\r\n\r\n\r\n// Load and chunk contents of blog\r\nconst pTagSelector = \"p\";\r\nconst cheerioLoader = new CheerioWebBaseLoader(\r\n  \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\r\n  {\r\n    selector: pTagSelector\r\n  }\r\n);\r\n\r\nconst docs = await cheerioLoader.load();\r\n\r\nconst splitter = new RecursiveCharacterTextSplitter({\r\n  chunkSize: 1000, chunkOverlap: 200\r\n});\r\nconst allSplits = await splitter.splitDocuments(docs);\r\n\r\n\r\n// Index chunks\r\nawait vectorStore.addDocuments(allSplits)\r\n\r\n// Define prompt for question-answering\r\nconst promptTemplate = await pull<ChatPromptTemplate>(\"rlm/rag-prompt\");\r\n\r\n// Define state for application\r\nconst InputStateAnnotation = Annotation.Root({\r\n  question: Annotation<string>,\r\n});\r\n\r\nconst StateAnnotation = Annotation.Root({\r\n  question: Annotation<string>,\r\n  context: Annotation<Document[]>,\r\n  answer: Annotation<string>,\r\n});\r\n\r\n// Define application steps\r\nconst retrieve = async (state: typeof InputStateAnnotation.State) => {\r\n  const retrievedDocs = await vectorStore.similaritySearch(state.question)\r\n  return { context: retrievedDocs };\r\n};\r\n\r\n\r\nconst generate = async (state: typeof StateAnnotation.State) => {\r\n  const docsContent = state.context.map(doc => doc.pageContent).join(\"\\n\");\r\n  const messages = await promptTemplate.invoke({ question: state.question, context: docsContent });\r\n  const response = await llm.invoke(messages);\r\n  return { answer: response.content };\r\n};\r\n\r\n\r\n// Compile application and test\r\nconst graph = new StateGraph(StateAnnotation)\r\n  .addNode(\"retrieve\", retrieve)\r\n  .addNode(\"generate\", generate)\r\n  .addEdge(\"__start__\", \"retrieve\")\r\n  .addEdge(\"retrieve\", \"generate\")\r\n  .addEdge(\"generate\", \"__end__\")\r\n  .compile();\n```\n\n```bash\nTask decomposition is the process of breaking down complex tasks into smaller, more manageable steps. This can be achieved through various methods, including prompting large language models (LLMs) or using task-specific instructions. Techniques like Chain of Thought (CoT) and Tree of Thoughts further enhance this process by structuring reasoning and exploring multiple possibilities at each step.\n```\n\nCheck out the [LangSmith trace](https://smith.langchain.com/public/84a36239-b466-41bd-ac84-befc33ab50df/r).\n\n## Detailed walkthrough\n\n### Indexing\n\n### Retrieval and Generation\n"
  },
  {
    "slug": "/orchestration/sql_qa",
    "title": "Build a Question/Answering system over SQL data",
    "description": "",
    "content": "<Note title=\"Prerequisites\" type=\"note\">\n  This guide assumes familiarity with the following concepts:\n\n  * Chat models\n  * Tools\n  * Agents\n  * LangGraph\n</Note>\n\nEnabling a LLM system to query structured data can be qualitatively different from unstructured text data. Whereas in the latter it is common to generate text that can be searched against a vector database, the approach for structured data is often for the LLM to write and execute queries in a DSL, such as SQL. In this guide we’ll go over the basic ways to create a Q\\&A system over tabular data in databases. We will cover implementations using both chains and agents. These systems will allow us to ask a question about the data in a database and get back a natural language answer. The main difference between the two is that our agent can query the database in a loop as many times as it needs to answer the question.\n\n## Security note\n\nBuilding Q\\&A systems of SQL databases requires executing model-generated SQL queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your chain/agent’s needs. This will mitigate though not eliminate the risks of building a model-driven system. For more on general security best practices, [see here](/docs/security).\n\n## Architecture\n\nAt a high-level, the steps of these systems are:\n\n1. Convert question to SQL query: Model converts user input to a SQL query.\n2. Execute SQL query: Execute the query.\n3. Answer the question: Model responds to user input using the query results.\n\n![sql\\_qa](/images/sql_qa.png \"sql_qa\")\n\n## Setup\n\nFirst, get required packages and set environment variables: **bash npm2yarn npm i langchain @langchain/community @langchain/langgraph**\n\n```bash\n# Uncomment the below to use LangSmith. Not required, but recommended for debugging and observability.\r\n# export LANGCHAIN_API_KEY=<your key>\r\n# export LANGCHAIN_TRACING_V2=true\r\n\r\n# Reduce tracing latency if you are not in a serverless environment\r\n# export LANGCHAIN_CALLBACKS_BACKGROUND=true\n```\n\n### Sample data\n\nThe below example will use a SQLite connection with the Chinook database, which is a sample database that represents a digital media store. Follow [these installation steps](https://database.guide/2-sample-databases-sqlite/) to create **Chinook.db** in the same directory as this notebook. You can also download and build the database via the command line:\n\n```bash\ncurl -s https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql | sqlite3 Chinook.db\n```\n\nNow, **Chinook.db** is in our directory and we can interface with it using the SqlDatabase class:\n\n```jsx {0} showLineNumbers\nimport { SqlDatabase } from \"langchain/sql_db\";\r\nimport { DataSource } from \"typeorm\";\r\n\r\nconst datasource = new DataSource({\r\n  type: \"sqlite\",\r\n  database: \"Chinook.db\",\r\n});\r\nconst db = await SqlDatabase.fromDataSourceParams({\r\n  appDataSource: datasource,\r\n});\r\n\r\nawait db.run(\"SELECT * FROM Artist LIMIT 10;\");\n```\n\n```bash\n[{\"ArtistId\":1,\"Name\":\"AC/DC\"},{\"ArtistId\":2,\"Name\":\"Accept\"},{\"ArtistId\":3,\"Name\":\"Aerosmith\"},{\"ArtistId\":4,\"Name\":\"Alanis Morissette\"},{\"ArtistId\":5,\"Name\":\"Alice In Chains\"},{\"ArtistId\":6,\"Name\":\"Antônio Carlos Jobim\"},{\"ArtistId\":7,\"Name\":\"Apocalyptica\"},{\"ArtistId\":8,\"Name\":\"Audioslave\"},{\"ArtistId\":9,\"Name\":\"BackBeat\"},{\"ArtistId\":10,\"Name\":\"Billy Cobham\"}]\n```\n\nGreat! We’ve got a SQL database that we can query. Now let’s try hooking it up to an LLM.\n\n## Chains\n\nChains are compositions of predictable steps. In LangGraph, we can represent a chain via simple sequence of nodes. Let’s create a sequence of steps that, given a question, does the following: - converts the question into a SQL query; - executes the query; - uses the result to answer the original question.\n\nThere are scenarios not supported by this arrangement. For example, this system will execute a SQL query for any user input– even “hello”. Importantly, as we’ll see below, some questions require more than one query to answer. We will address these scenarios in the Agents section.\n\n### Application state\n\nThe LangGraph state of our application controls what data is input to the application, transferred between steps, and output by the application.\n\nFor this application, we can just keep track of the input question, generated query, query result, and generated answer:\n\n```jsx {0} showLineNumbers\nimport { Annotation } from \"@langchain/langgraph\";\r\n\r\nconst InputStateAnnotation = Annotation.Root({\r\n  question: Annotation<string>,\r\n});\r\n\r\nconst StateAnnotation = Annotation.Root({\r\n  question: Annotation<string>,\r\n  query: Annotation<string>,\r\n  result: Annotation<string>,\r\n  answer: Annotation<string>,\r\n});\n```\n\nNow we just need functions that operate on this state and populate its contents.\n"
  },
  {
    "slug": "/orchestration/summarization",
    "title": "Summarize Text",
    "description": "",
    "content": "<Note title=\"INFO\" type=\"note\">\n  This tutorial demonstrates text summarization using built-in chains and [LangGraph](https://langchain-ai.github.io/langgraphjs/).\n\n  See here for a previous version of this page, which showcased the legacy chain [RefineDocumentsChain](https://v03.api.js.langchain.com/classes/langchain.chains.RefineDocumentsChain.html).\n</Note>\n\nSuppose you have a set of documents (PDFs, Notion pages, customer questions, etc.) and you want to summarize the content.\n\nLLMs are a great tool for this given their proficiency in understanding and synthesizing text.\n\nIn the context of [retrieval-augmented generation](/docs/tutorials/rag), summarizing text can help distill the information in a large number of retrieved documents to provide context for a LLM.\n\nIn this walkthrough we’ll go over how to summarize content from multiple documents using LLMs.\n\n## Concepts\n\nConcepts we will cover are:\n\n* Using language models.\n\n* Using document loaders, specifically the CheerioWebBaseLoader to load content from an HTML webpage.\n\n* Two ways to summarize or otherwise combine documents.\n\n  * Stuff, which simply concatenates documents into a prompt;\n  * Map-reduce, for larger sets of documents. This splits documents into batches, summarizes those, and then summarizes the summaries.\n\n## Setup\n\n### Jupyter Notebook\n\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebooks. Going through guides in an interactive environment is a great way to better understand them. See here for instructions on how to install.\n\n### Installation\n\nTo install LangChain run:\n\n```bash\nbash npm2yarn npm i langchain @langchain/core\n```\n\nFor more details, see [our Installation guide](/docs/introduction/installation).\n\n### LangSmith\n\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\n\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\n\n```bash\nexport LANGCHAIN_TRACING_V2=\"true\"\r\nexport LANGCHAIN_API_KEY=\"...\"\r\n\r\n# Reduce tracing latency if you are not in a serverless environment\r\n# export LANGCHAIN_CALLBACKS_BACKGROUND=true\n```\n\n## Overview\n\nA central question for building a summarizer is how to pass your documents into the LLM’s context window. Two common approaches for this are:\n\n1. **Stuff**: Simply “stuff” all your documents into a single prompt. This is the simplest approach.\n\n2. **Map-reduce**: Summarize each document on its own in a “map” step and then “reduce” the summaries into a final summary.\n\nNote that map-reduce is especially effective when understanding of a sub-document does not rely on preceding context. For example, when summarizing a corpus of many, shorter documents. In other cases, such as summarizing a novel or body of text with an inherent sequence, iterative refinement may be more effective.\n\nFirst we load in our documents. We will use WebBaseLoader to load a blog post:\n\n```jsx {0} showLineNumbers\nimport \"cheerio\";\r\nimport { CheerioWebBaseLoader } from \"@langchain/community/document_loaders/web/cheerio\";\r\n\r\nconst pTagSelector = \"p\";\r\nconst cheerioLoader = new CheerioWebBaseLoader(\r\n  \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\r\n  {\r\n    selector: pTagSelector,\r\n  }\r\n);\r\n\r\nconst docs = await cheerioLoader.load();\n```\n\nLet’s next select a chat model:\n\n### Pick your chat model:\n\nInstall dependencies:\n\n<Note title=\"TIP\" type=\"success\">\n  See [this section for general instructions on installing integration\r\n  packages](/docs/introduction/installation#installation).\n</Note>\n\n## Stuff: summarize in a single LLM call\n\nWe can use createStuffDocumentsChain, especially if using larger context window models such as:\n\n* 128k token OpenAI gpt-4o\n* 200k token Anthropic claude-3-5-sonnet-20240620\n\nThe chain will take a list of documents, insert them all into a prompt, and pass that prompt to an LLM:\n\n```jsx {0} showLineNumbers\nimport { createStuffDocumentsChain } from \"langchain/chains/combine_documents\";\r\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\r\nimport { PromptTemplate } from \"@langchain/core/prompts\";\r\n\r\n// Define prompt\r\nconst prompt = PromptTemplate.fromTemplate(\r\n  \"Summarize the main themes in these retrieved docs: {context}\"\r\n);\r\n\r\n// Instantiate\r\nconst chain = await createStuffDocumentsChain({\r\n  llm: llm,\r\n  outputParser: new StringOutputParser(),\r\n  prompt,\r\n});\r\n\r\n// Invoke\r\nconst result = await chain.invoke({ context: docs });\r\nconsole.log(result);\n```\n\n```bash\nThe retrieved documents discuss the development and capabilities of autonomous agents powered by large language models (LLMs). Here are the main themes:\r\n\r\n1. **LLM as a Core Controller**: LLMs are positioned as the central intelligence in autonomous agent systems, capable of performing complex tasks beyond simple text generation. They can be framed as general problem solvers, with various implementations like AutoGPT, GPT-Engineer, and BabyAGI serving as proof-of-concept demonstrations.\r\n\r\n2. **Task Decomposition and Planning**: Effective task management is crucial for LLMs. Techniques like Chain of Thought (CoT) and Tree of Thoughts (ToT) are highlighted for breaking down complex tasks into manageable steps. CoT encourages step-by-step reasoning, while ToT explores multiple reasoning paths, enhancing the agent's problem-solving capabilities.\r\n\r\n3. **Integration of External Tools**: The use of external tools significantly enhances LLM capabilities. Frameworks like MRKL and Toolformer allow LLMs to interact with various APIs and tools, improving their performance in specific tasks. This modular approach enables LLMs to route inquiries to specialized modules, combining neural and symbolic reasoning.\r\n\r\n4. **Self-Reflection and Learning**: Self-reflection mechanisms are essential for agents to learn from past actions and improve over time. Approaches like ReAct and Reflexion integrate reasoning with action, allowing agents to evaluate their performance and adjust strategies based on feedback.\r\n\r\n5. **Memory and Context Management**: The documents discuss different types of memory (sensory, short-term, long-term) and their relevance to LLMs. The challenge of finite context length in LLMs is emphasized, as it limits the ability to retain and utilize historical information effectively. Techniques like external memory storage and vector databases are suggested to mitigate these limitations.\r\n\r\n6. **Challenges and Limitations**: Several challenges are identified, including the reliability of natural language interfaces, difficulties in long-term planning, and the need for robust task decomposition. The documents note that LLMs may struggle with unexpected errors and formatting issues, which can hinder their performance in real-world applications.\r\n\r\n7. **Emerging Applications**: The potential applications of LLM-powered agents are explored, including scientific discovery, autonomous design, and interactive simulations (e.g., generative agents mimicking human behavior). These applications demonstrate the versatility and innovative possibilities of LLMs in various domains.\r\n\r\nOverall, the documents present a comprehensive overview of the current state of LLM-powered autonomous agents, highlighting their capabilities, methodologies, and the challenges they face in practical implementations.\n```\n\n### Streaming\n\nNote that we can also stream the result token-by-token:\n\n```jsx {0} showLineNumbers\nconst stream = await chain.stream({ context: docs });\r\n\r\nfor await (const token of stream) {\r\n  process.stdout.write(token + \"|\");\r\n}\n```\n\n```bash\n|The| retrieved| documents| discuss| the| development| and| capabilities| of| autonomous| agents| powered| by| large| language| models| (|LL|Ms|).| Here| are| the| main| themes|:\r\n\r\n|1|.| **|LL|M| as| a| Core| Controller|**|:| L|LM|s| are| positioned| as| the| central| intelligence| in| autonomous| agent| systems|,| capable| of| performing| complex| tasks| beyond| simple| text| generation|.| They| can| be| framed| as| general| problem| sol|vers|,| with| various| implementations| like| Auto|GPT|,| GPT|-|Engineer|,| and| Baby|AG|I| serving| as| proof|-of|-con|cept| demonstrations|.\r\n\r\n|2|.| **|Task| De|composition| and| Planning|**|:| Effective| task| management| is| crucial| for| L|LM|s| to| handle| complicated| tasks|.| Techniques| like| Chain| of| Thought| (|Co|T|)| and| Tree| of| Thoughts| (|To|T|)| are| highlighted| for| breaking| down| tasks| into| manageable| steps| and| exploring| multiple| reasoning| paths|.| Additionally|,| L|LM|+|P| integrates| classical| planning| methods| to| enhance| long|-term| planning| capabilities|.\r\n\r\n|3|.| **|Self|-|Reflection| and| Learning|**|:| Self|-ref|lection| mechanisms| are| essential| for| agents| to| learn| from| past| actions| and| improve| their| decision|-making| processes|.| Framework|s| like| Re|Act| and| Reflex|ion| incorporate| dynamic| memory| and| self|-ref|lection| to| refine| reasoning| skills| and| enhance| performance| through| iterative| learning|.\r\n\r\n|4|.| **|Tool| Util|ization|**|:| The| integration| of| external| tools| significantly| extends| the| capabilities| of| L|LM|s|.| Appro|aches| like| MR|KL| and| Tool|former| demonstrate| how| L|LM|s| can| be| augmented| with| various| APIs| to| perform| specialized| tasks|,| enhancing| their| functionality| in| real|-world| applications|.\r\n\r\n|5|.| **|Memory| and| Context| Management|**|:| The| documents| discuss| different| types| of| memory| (|sens|ory|,| short|-term|,| long|-term|)| and| their| relevance| to| L|LM|s|.| The| challenge| of| finite| context| length| is| emphasized|,| as| it| limits| the| model|'s| ability| to| retain| and| utilize| historical| information| effectively|.| Techniques| like| vector| stores| and| approximate| nearest| neighbors| (|ANN|)| are| suggested| to| improve| retrieval| speed| and| memory| management|.\r\n\r\n|6|.| **|Challenges| and| Limit|ations|**|:| Several| limitations| of| current| L|LM|-powered| agents| are| identified|,| including| issues| with| the| reliability| of| natural| language| interfaces|,| difficulties| in| long|-term| planning|,| and| the| need| for| improved| efficiency| in| task| execution|.| The| documents| also| highlight| the| importance| of| human| feedback| in| refining| model| outputs| and| addressing| potential| biases|.\r\n\r\n|7|.| **|Emer|ging| Applications|**|:| The| potential| applications| of| L|LM|-powered| agents| are| explored|,| including| scientific| discovery|,| autonomous| design|,| and| interactive| simulations| (|e|.g|.,| gener|ative| agents|).| These| applications| showcase| the| versatility| of| L|LM|s| in| various| domains|,| from| drug| discovery| to| social| behavior| simulations|.\r\n\r\n|Overall|,| the| documents| present| a| comprehensive| overview| of| the| current| state| of| L|LM|-powered| autonomous| agents|,| their| capabilities|,| methodologies| for| improvement|,| and| the| challenges| they| face| in| practical| applications|.|||\n```\n\n### Go deeper\n\n* You can easily customize the prompt.\n* You can easily try different LLMs, (e.g., Claude) via the llm parameter.\n"
  },
  {
    "slug": "/random",
    "title": "Introduction",
    "description": "This section provides an overview of Introduction.",
    "content": "Lorem ipsum dolor sit amet consectetur adipisicing elit. Numquam iste dolorum tempore consectetur explicabo tempora provident quia maxime, molestiae, facilis aperiam et, error illum vel ullam? Quis architecto dolore ullam\n\n* \\[x] Write the press release\n* \\[ ] Update the website\n* \\[ ] Contact the media\n\n| Syntax        | Description |   Test Text |\r\n| :------------ | :---------: | ----------: |\r\n| Header        |    Title    | Here's this |\r\n| Paragraph     |    Text     |    And more |\r\n| Strikethrough |             |    ~~Text~~ |\n\n## Getting Started\n\nTo begin using the Documentation Template, follow these simple steps:\n\n* Start by cloning the repository to your local machine.\n\nLorem ipsum dolor sit amet consectetur adipisicing elit. Reprehenderit quae iure nulla deserunt dolore quam pariatur minus sapiente accusantium. Optio, necessitatibus sequi. Veritatis, aspernatur? Possimus quis repellat eum vitae eveniet.\n\n## Blockquotes\n\nBlockquotes are useful for emphasizing key points or quoting external sources:\n\n> \"Documentation is a love letter that you write to your future self.\" - Damian Conway\n\nFeel free to use blockquotes to highlight important information or quotes relevant to your documentation.\n\n## Code Examples with switch\n\nHere a custom tab component from shadcn ui is used.\n\n## Conclusion\n\nAdding some random stuff to change the code\n"
  },
  {
    "slug": "/security",
    "title": "Security",
    "description": "LangChain has a large ecosystem of integrations with various external resources like local and remote file systems, APIs and databases. These integrations allow developers to create versatile applications that combine the power of LLMs with the ability to access, interact with and manipulate external resources.",
    "content": "## Best Practices\n\nWhen building such applications developers should remember to follow good security practices:\n\n* Limit Permissions: Scope permissions specifically to the application's need. Granting broad or excessive permissions can introduce significant security vulnerabilities. To avoid such vulnerabilities, consider using read-only credentials, disallowing access to sensitive resources, using sandboxing techniques (such as running inside a container), etc. as appropriate for your application.\n* Anticipate Potential Misuse: Just as humans can err, so can Large Language Models (LLMs). Always assume that any system access or credentials may be used in any way allowed by the permissions they are assigned. For example, if a pair of database credentials allows deleting data, it’s safest to assume that any LLM able to use those credentials may in fact delete data.\n* Defense in Depth: No security technique is perfect. Fine-tuning and good chain design can reduce, but not eliminate, the odds that a Large Language Model (LLM) may make a mistake. It’s best to combine multiple layered security approaches rather than relying on any single layer of defense to ensure security. For example: use both read-only permissions and sandboxing to ensure that LLMs are only able to access data that is explicitly meant for them to use.\n\nRisks of not doing so include, but are not limited to:\n\n* Data corruption or loss.\n* Unauthorized access to confidential information.\n* Compromised performance or availability of critical resources.\n\nExample scenarios with mitigation strategies:\n\n* A user may ask an agent with access to the file system to delete files that should not be deleted or read the content of files that contain sensitive information. To mitigate, limit the agent to only use a specific directory and only allow it to read or write files that are safe to read or write. Consider further sandboxing the agent by running it in a container.\n* A user may ask an agent with write access to an external API to write malicious data to the API, or delete data from that API. To mitigate, give the agent read-only API keys, or limit it to only use endpoints that are already resistant to such misuse.\n* A user may ask an agent with access to a database to drop a table or mutate the schema. To mitigate, scope the credentials to only the tables that the agent needs to access and consider issuing READ-ONLY credentials.\n\nIf you're building applications that access external resources like file systems, APIs or databases, consider speaking with your company's security team to determine how to best design and secure your applications.\n\n## Reporting a Vulnerability\n\nPlease report security vulnerabilities by email to security@langchain.dev. This will ensure the issue is promptly triaged and acted upon as needed.\n"
  },
  {
    "slug": "/structure/deeper/even-deeper",
    "title": "Introduction",
    "description": "This section provides an overview of Introduction.",
    "content": "Lorem ipsum dolor sit amet consectetur adipisicing elit. Numquam iste dolorum tempore consectetur explicabo tempora provident quia maxime, molestiae, facilis aperiam et, error illum vel ullam? Quis architecto dolore ullam\n\n* \\[x] Write the press release\n* \\[ ] Update the website\n* \\[ ] Contact the media\n\n| Syntax        | Description |   Test Text |\r\n| :------------ | :---------: | ----------: |\r\n| Header        |    Title    | Here's this |\r\n| Paragraph     |    Text     |    And more |\r\n| Strikethrough |             |    ~~Text~~ |\n\n<CardGrid>\n  <Card title=\"Page Structure\" href=\"/docs/deep/deeper\" icon=\"alignJustify\" variant=\"small\" description=\"test description\" />\n\n  <Card title=\"Page Structure\" href=\"/docs/deep/deeper\" icon=\"alignJustify\" variant=\"small\" />\n\n  <Card title=\"Rubix Studios\" href=\"https://www.rubixstudios.com.au\" icon=\"alignJustify\" external={true} variant=\"small\" />\n\n  <Card title=\"Rubix Studios\" href=\"https://www.rubixstudios.com.au\" image=\"/images/og-image.png\" external={true} variant=\"image\" />\n</CardGrid>\n"
  },
  {
    "slug": "/structure/deeper",
    "title": "Introduction",
    "description": "This section provides an overview of Introduction.",
    "content": "Lorem ipsum dolor sit amet consectetur adipisicing elit. Numquam iste dolorum tempore consectetur explicabo tempora provident quia maxime, molestiae, facilis aperiam et, error illum vel ullam? Quis architecto dolore ullam\n\n* \\[x] Write the press release\n* \\[ ] Update the website\n* \\[ ] Contact the media\n\n| Syntax        | Description |   Test Text |\r\n| :------------ | :---------: | ----------: |\r\n| Header        |    Title    | Here's this |\r\n| Paragraph     |    Text     |    And more |\r\n| Strikethrough |             |    ~~Text~~ |\n\n## Getting Started\n\nTo begin using the Documentation Template, follow these simple steps:\n\n* Start by cloning the repository to your local machine.\n\nLorem ipsum dolor sit amet consectetur adipisicing elit. Reprehenderit quae iure nulla deserunt dolore quam pariatur minus sapiente accusantium. Optio, necessitatibus sequi. Veritatis, aspernatur? Possimus quis repellat eum vitae eveniet.\n\n## Blockquotes\n\nBlockquotes are useful for emphasizing key points or quoting external sources:\n\n> \"Documentation is a love letter that you write to your future self.\" - Damian Conway\n\nFeel free to use blockquotes to highlight important information or quotes relevant to your documentation.\n\n## Code Examples with switch\n\nHere a custom tab component from shadcn ui is used.\n\n## Conclusion\n\nThank you for choosing the Documentation Template for your project. Whether you're documenting software, APIs, or processes, we're here to support you in creating clear and effective documentation. Happy documenting!\n\n## Tabs Example\n"
  },
  {
    "slug": "/structure",
    "title": "Structure",
    "description": "This section provides an overview of Introduction.",
    "content": "Lorem ipsum dolor sit amet consectetur adipisicing elit. Numquam iste dolorum tempore consectetur explicabo tempora provident quia maxime, molestiae, facilis aperiam et, error illum vel ullam? Quis architecto dolore ullam\n\n* \\[x] Write the press release\n* \\[ ] Update the website\n* \\[ ] Contact the media\n\n| Syntax        | Description |   Test Text |\r\n| :------------ | :---------: | ----------: |\r\n| Header        |    Title    | Here's this |\r\n| Paragraph     |    Text     |    And more |\r\n| Strikethrough |             |    ~~Text~~ |\n\n## Getting Started\n\nTo begin using the Documentation Template, follow these simple steps:\n\n* Start by cloning the repository to your local machine.\n\nLorem ipsum dolor sit amet consectetur adipisicing elit. Reprehenderit quae iure nulla deserunt dolore quam pariatur minus sapiente accusantium. Optio, necessitatibus sequi. Veritatis, aspernatur? Possimus quis repellat eum vitae eveniet.\n\n## Blockquotes\n\nBlockquotes are useful for emphasizing key points or quoting external sources:\n\n> \"Documentation is a love letter that you write to your future self.\" - Damian Conway\n\nFeel free to use blockquotes to highlight important information or quotes relevant to your documentation.\n\n## Code Examples with switch\n\nHere a custom tab component from shadcn ui is used.\n\n## Conclusion\n\nThank you for choosing the Documentation Template for your project. Whether you're documenting software, APIs, or processes, we're here to support you in creating clear and effective documentation. Happy documenting!\n"
  },
  {
    "slug": "/tutorials/classification",
    "title": "Classify Text into Labels",
    "description": "",
    "content": "Tagging means labeling a document with classes such as:\n\n* sentiment\n* language\n* style (formal, informal etc.)\n* covered topics\n* political tendency\n\n![Banner](/images/classification.png \"Documents\")\n\n## Overview\n\nTagging has a few components:\n\n* **function**: Like **extraction**, tagging uses functions to specify how the model should tag a document\n* **schema**: defines how we want to tag the document\n\n## Quickstart\n\nLet’s see a very straightforward example of how we can use tool calling for tagging in LangChain. We’ll use the **.withStructuredOutput()**, supported on **selected chat models**.\n\n### Pick your chat model:\n\nInstall dependencies:\n\n<Note title=\"TIP\" type=\"success\">\n  See [this section for general instructions on installing integration\r\n  packages](/docs/introduction/installation#installation).\n</Note>\n\nLet’s specify a Zod schema with a few properties and their expected type in our schema.\n\n```jsx {0} showLineNumbers\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\r\nimport { z } from \"zod\";\r\n\r\nconst taggingPrompt = ChatPromptTemplate.fromTemplate(\r\n  `Extract the desired information from the following passage.\r\n\r\nOnly extract the properties mentioned in the 'Classification' function.\r\n\r\nPassage:\r\n{input}\r\n`\r\n);\r\n\r\nconst classificationSchema = z.object({\r\n  sentiment: z.string().describe(\"The sentiment of the text\"),\r\n  aggressiveness: z\r\n    .number()\r\n    .int()\r\n    .min(1)\r\n    .max(10)\r\n    .describe(\"How aggressive the text is on a scale from 1 to 10\"),\r\n  language: z.string().describe(\"The language the text is written in\"),\r\n});\r\n\r\n// Name is optional, but gives the models more clues as to what your schema represents\r\nconst llmWihStructuredOutput = llm.withStructuredOutput(classificationSchema, {\r\n  name: \"extractor\",\r\n});\n```\n\n```jsx {0} showLineNumbers\nconst prompt1 = await taggingPrompt.invoke({\r\n  input:\r\n    \"Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!\",\r\n});\r\nawait llmWihStructuredOutput.invoke(prompt1);\n```\n\n```bash\n{ sentiment: 'contento', aggressiveness: 1, language: 'es' }\n```\n\nAs we can see in the example, it correctly interprets what we want.\n\nThe results vary so that we may get, for example, sentiments in different languages (‘positive’, ‘enojado’ etc.).\n\nWe will see how to control these results in the next section.\n\n## Finer control\n\nCareful schema definition gives us more control over the model’s output.\n\nSpecifically, we can define:\n\n* possible values for each property\n* description to make sure that the model understands the property\n* required properties to be returned\n\nLet’s redeclare our Zod schema to control for each of the previously mentioned aspects using enums:\n\n```jsx {0} showLineNumbers\nimport { z } from \"zod\";\r\n\r\nconst classificationSchema2 = z.object({\r\n  sentiment: z\r\n    .enum([\"happy\", \"neutral\", \"sad\"])\r\n    .describe(\"The sentiment of the text\"),\r\n  aggressiveness: z\r\n    .number()\r\n    .int()\r\n    .min(1)\r\n    .max(5)\r\n    .describe(\r\n      \"describes how aggressive the statement is, the higher the number the more aggressive\"\r\n    ),\r\n  language: z\r\n    .enum([\"spanish\", \"english\", \"french\", \"german\", \"italian\"])\r\n    .describe(\"The language the text is written in\"),\r\n});\n```\n\n```jsx {0} showLineNumbers\nconst taggingPrompt2 = ChatPromptTemplate.fromTemplate(\r\n  `Extract the desired information from the following passage.\r\n\r\nOnly extract the properties mentioned in the 'Classification' function.\r\n\r\nPassage:\r\n{input}\r\n`\r\n);\r\n\r\nconst llmWihStructuredOutput2 = llm.withStructuredOutput(\r\n  classificationSchema2,\r\n  { name: \"extractor\" }\r\n);\n```\n\nNow the answers will be restricted in a way we expect!\n\n```jsx {0} showLineNumbers\nconst prompt2 = await taggingPrompt2.invoke({\r\n  input:\r\n    \"Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!\",\r\n});\r\nawait llmWihStructuredOutput2.invoke(prompt2);\n```\n\n```bash\n{ sentiment: 'happy', aggressiveness: 1, language: 'spanish' }\n```\n\n```jsx {0} showLineNumbers\nconst prompt3 = await taggingPrompt2.invoke({\r\n  input: \"Estoy muy enojado con vos! Te voy a dar tu merecido!\",\r\n});\r\nawait llmWihStructuredOutput2.invoke(prompt3);\n```\n\n```bash\n{ sentiment: 'sad', aggressiveness: 5, language: 'spanish' }\n```\n\n```jsx {0} showLineNumbers\nconst prompt4 = await taggingPrompt2.invoke({\r\n  input: \"Weather is ok here, I can go outside without much more than a coat\",\r\n});\r\nawait llmWihStructuredOutput2.invoke(prompt4);\n```\n\n```bash\n{ sentiment: 'neutral', aggressiveness: 1, language: 'english' }\n```\n\nThe [LangSmith trace](https://smith.langchain.com/public/455f5404-8784-49ce-8851-0619b99e936f/r) lets us peek under the hood:\n\n![LangSmith trace](/images/langsmith_trace.png \"LangSmith trace\")\n"
  },
  {
    "slug": "/tutorials/extraction",
    "title": "Build an Extraction Chain",
    "description": "",
    "content": "<Note title=\"Prerequisites\" type=\"note\">\n  This guide assumes familiarity with the following concepts: - Chat Models -\r\n  Tools - Tool calling\n</Note>\n\nIn this tutorial, we will build a chain to extract structured information from unstructured text.\n\n<Note title=\"INFO\" type=\"note\">\n  This tutorial will only work with models that support **function/tool\r\n  calling**\n</Note>\n\n## Setup\n\n### Installation\n\nTo install LangChain run:\n\nFor more details, see our [**Installation guide**](/docs/introduction/installation).\n\n### LangSmith\n\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\n\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\n\n```bash\nexport LANGCHAIN_TRACING_V2=\"true\"\r\nexport LANGCHAIN_API_KEY=\"...\"\r\n\r\n# Reduce tracing latency if you are not in a serverless environment\r\n# export LANGCHAIN_CALLBACKS_BACKGROUND=true\n```\n\n## The Schema\n\nFirst, we need to describe what information we want to extract from the text.\n\nWe’ll use Zod to define an example schema that extracts personal information.\n\n```jsx {0} showLineNumbers\nimport { z } from \"zod\";\r\n\r\nconst personSchema = z.object({\r\n  name: z.optional(z.string()).describe(\"The name of the person\"),\r\n  hair_color: z\r\n    .optional(z.string())\r\n    .describe(\"The color of the person's hair if known\"),\r\n  height_in_meters: z\r\n    .optional(z.string())\r\n    .describe(\"Height measured in meters\"),\r\n});\n```\n\nThere are two best practices when defining schema:\n\n1. Document the **attributes** and the **schema** itself: This information is sent to the LLM and is used to improve the quality of information extraction.\n2. Do not force the LLM to make up information! Above we used **.nullish()** for the attributes allowing the LLM to output **null** or **undefined** if it doesn’t know the answer.\n\n<Note title=\"INFO\" type=\"note\">\n  For best performance, document the schema well and make sure the model isn’t\r\n  force to return results if there’s no information to be extracted in the text.\n</Note>\n\n## The Extractor\n\nLet’s create an information extractor using the schema we defined above.\n\n```jsx {0} showLineNumbers\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\r\n\r\n// Define a custom prompt to provide instructions and any additional context.\r\n// 1) You can add examples into the prompt template to improve extraction quality\r\n// 2) Introduce additional parameters to take context into account (e.g., include metadata\r\n//    about the document from which the text was extracted.)\r\nconst promptTemplate = ChatPromptTemplate.fromMessages([\r\n  [\r\n    \"system\",\r\n    `You are an expert extraction algorithm.\r\nOnly extract relevant information from the text.\r\nIf you do not know the value of an attribute asked to extract,\r\nreturn null for the attribute's value.`,\r\n  ],\r\n  // Please see the how-to about improving performance with\r\n  // reference examples.\r\n  // [\"placeholder\", \"{examples}\"],\r\n  [\"human\", \"{text}\"],\r\n]);\n```\n\nWe need to use a model that supports function/tool calling.\n\nPlease review [the documentation](/docs/integrations/chat) for list of some models that can be used with this API.\n\nWe enable structured output by creating a new object with the **.withStructuredOutput** method:\n\n```jsx {0} showLineNumbers\nconst structured_llm = llm.withStructuredOutput(personSchema);\n```\n\nWe can then invoke it normally:\n\n```jsx {0} showLineNumbers\nconst prompt = await promptTemplate.invoke({\r\n  text: \"Alan Smith is 6 feet tall and has blond hair.\",\r\n});\r\nawait structured_llm.invoke(prompt);\n```\n\n```bash\n{ name: 'Alan Smith', hair_color: 'blond', height_in_meters: '1.83' }\n```\n\n<Note title=\"INFO\" type=\"note\">\n  Extraction is Generative 🤯\n\n  LLMs are generative models, so they can do some pretty cool things like correctly extract the height of the person in meters even though it was provided in feet!\n</Note>\n\nWe can see the LangSmith trace here.\n\nEven though we defined our schema with the variable name **personSchema**, Zod is unable to infer this name and therefore does not pass it along to the model. To help give the LLM more clues as to what your provided schema represents, you can also give the schema you pass to **withStructuredOutput()** a name:\n\n```jsx {0} showLineNumbers\nconst structured_llm2 = llm.withStructuredOutput(personSchema, {\r\n  name: \"person\",\r\n});\r\n\r\nconst prompt2 = await promptTemplate.invoke({\r\n  text: \"Alan Smith is 6 feet tall and has blond hair.\",\r\n});\r\nawait structured_llm2.invoke(prompt2);\n```\n\n```bash\n{ name: 'Alan Smith', hair_color: 'blond', height_in_meters: '1.83' }\n```\n\nThis can improve performance in many cases.\n\n## Multiple Entities\n\nIn most cases, you should be extracting a list of entities rather than a single entity.\n\nThis can be easily achieved using Zod by nesting models inside one another.\n\n```jsx {0} showLineNumbers\nimport { z } from \"zod\";\r\n\r\nconst person = z.object({\r\n  name: z.optional(z.string()).describe(\"The name of the person\"),\r\n  hair_color: z\r\n    .optional(z.string())\r\n    .describe(\"The color of the person's hair if known\"),\r\n  height_in_meters: z.number().nullish().describe(\"Height measured in meters\"),\r\n});\r\n\r\nconst dataSchema = z.object({\r\n  people: z.array(person).describe(\"Extracted data about people\"),\r\n});\n```\n\n<Note title=\"INFO\" type=\"note\">\n  Extraction might not be perfect here. Please continue to see how to use Reference Examples to improve the quality of extraction, and see the guidelines section!\n</Note>\n\n```jsx {0} showLineNumbers\nconst structured_llm3 = llm.withStructuredOutput(dataSchema);\r\nconst prompt3 = await promptTemplate.invoke({\r\n  text: \"My name is Jeff, my hair is black and i am 6 feet tall. Anna has the same color hair as me.\",\r\n});\r\nawait structured_llm3.invoke(prompt3);\n```\n\n```bash\n{\r\n  people: [\r\n    { name: 'Jeff', hair_color: 'black', height_in_meters: 1.83 },\r\n    { name: 'Anna', hair_color: 'black', height_in_meters: null }\r\n  ]\r\n}\n```\n\n<Note title=\"TIP\" type=\"success\">\n  When the schema accommodates the extraction of multiple entities, it also allows the model to extract no entities if no relevant information is in the text by providing an empty list.\n\n  This is usually a good thing! It allows specifying required attributes on an entity without necessarily forcing the model to detect this entity.\n</Note>\n\nWe can see the LangSmith trace [here](https://smith.langchain.com/public/272096ab-9ac5-43f9-aa00-3b8443477d17/r)\n\n## Next steps\n\nNow that you understand the basics of extraction with LangChain, you’re ready to proceed to the rest of the how-to guides:\n\n* [Add Examples](/docs/how_to/extraction_examples): Learn how to use reference examples to improve performance.\n* [Handle Long Text](/docs/how_to/extraction_long_text): What should you do if the text does not fit into the context window of the LLM?\n* [Use a Parsing Approach](/docs/how_to/extraction_parse): Use a prompt based approach to extract with models that do not support tool/function calling.\n"
  },
  {
    "slug": "/tutorials",
    "title": "Tutorials",
    "description": "New to LangChain or LLM app development in general? Read this material to quickly get up and running building your first applications.",
    "content": "## Get started\n\nFamiliarize yourself with LangChain's open-source components by building simple applications.\n\nIf you're looking to get started with chat models, vector stores, or other LangChain components from a specific provider, check out our supported integrations.\n\n* **[Chat models and prompts](/docs/tutorials/llm_chain)**: Build a simple LLM application with prompt templates and chat models.\n* **[Semantic search](/docs/tutorials/retrievers)**: Build a semantic search engine over a PDF with document loaders, embedding models, and vector stores.\n* **[Classification](/docs/tutorials/classification)**: Classify text into categories or labels using chat models with structured outputs.\n* **[Extraction](/docs/tutorials/extraction)**: Extract structured data from text and other unstructured media using chat models and few-shot examples.\n\nRefer to the how-to guides for more detail on using all LangChain components.\n\n## Orchestration\n\nGet started using LangGraph to assemble LangChain components into full-featured applications.\n\n* **Chatbots**: Build a chatbot that incorporates memory.\n* **Agents**: Build an agent with LangGraph.js that interacts with external tools.\n* **Retrieval Augmented Generation (RAG) Part 1**: Build an application that uses your own documents to inform its responses.\n* **Retrieval Augmented Generation (RAG) Part 2**: Build a RAG application that incorporates a memory of its user interactions and multi-step retrieval.\n* **Question-Answering with SQL**: Build a question-answering system that executes SQL queries to inform its responses.\n* **Summarization**: Generate summaries of (potentially long) texts.\n* **Question-Answering with Graph Databases**: Build a question-answering system that queries a graph database to inform its responses.\n\n## LangSmith\n\nLangSmith allows you to closely trace, monitor and evaluate your LLM application. It seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of your chains as you build.\n\nLangSmith documentation is hosted on a separate site. You can peruse **[LangSmith tutorials here](https://docs.smith.langchain.com/)**.\n\n### Evaluation\n\nLangSmith helps you evaluate the performance of your LLM applications. The below tutorial is a great way to get started:\n\n* **[Evaluate your LLM application](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)**\n"
  },
  {
    "slug": "/tutorials/llm_chain",
    "title": "Build a simple LLM application with chat models and prompt templates",
    "description": "In this quickstart we’ll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it’s just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!",
    "content": "After reading this tutorial, you’ll have a high level overview of:\n\n* Using language models\n\n* Using prompt templates\n\n* Debugging and tracing your application using LangSmith\n\nLet’s dive in!\n\n## Setup\n\n### Installation\n\nTo install LangChain run:\n\nFor more details, see our [Installation guide](/docs/introduction/installation).\n\n### LangSmith\n\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\n\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\n\n```bash\nexport LANGCHAIN_TRACING_V2=\"true\"\r\nexport LANGCHAIN_API_KEY=\"...\"\r\n\r\n# Reduce tracing latency if you are not in a serverless environment\r\n# export LANGCHAIN_CALLBACKS_BACKGROUND=true\n```\n\n## Using Language Models\n\nFirst up, let’s learn how to use a language model by itself. LangChain supports many different language models that you can use interchangeably. For details on getting started with a specific model, refer to supported integrations.\n\n### Pick your chat model:\n\nLet’s first use the model directly. ChatModels are instances of LangChain Runnables, which means they expose a standard interface for interacting with them. To simply call the model, we can pass in a list of messages to the **.invoke** method.\n\n```jsx {0} showLineNumbers\nimport { HumanMessage, SystemMessage } from \"@langchain/core/messages\";\r\n\r\nconst messages = [\r\n  new SystemMessage(\"Translate the following from English into Italian\"),\r\n  new HumanMessage(\"hi!\"),\r\n];\r\n\r\nawait model.invoke(messages);\n```\n\n```bash\nAIMessage {\r\n  \"id\": \"chatcmpl-AekSfJkg3QIOsk42BH6Qom4Gt159j\",\r\n  \"content\": \"Ciao!\",\r\n  \"additional_kwargs\": {},\r\n  \"response_metadata\": {\r\n    \"tokenUsage\": {\r\n      \"promptTokens\": 20,\r\n      \"completionTokens\": 3,\r\n      \"totalTokens\": 23\r\n    },\r\n    \"finish_reason\": \"stop\",\r\n    \"usage\": {\r\n      \"prompt_tokens\": 20,\r\n      \"completion_tokens\": 3,\r\n      \"total_tokens\": 23,\r\n      \"prompt_tokens_details\": {\r\n        \"cached_tokens\": 0,\r\n        \"audio_tokens\": 0\r\n      },\r\n      \"completion_tokens_details\": {\r\n        \"reasoning_tokens\": 0,\r\n        \"audio_tokens\": 0,\r\n        \"accepted_prediction_tokens\": 0,\r\n        \"rejected_prediction_tokens\": 0\r\n      }\r\n    },\r\n    \"system_fingerprint\": \"fp_6fc10e10eb\"\r\n  },\r\n  \"tool_calls\": [],\r\n  \"invalid_tool_calls\": [],\r\n  \"usage_metadata\": {\r\n    \"output_tokens\": 3,\r\n    \"input_tokens\": 20,\r\n    \"total_tokens\": 23,\r\n    \"input_token_details\": {\r\n      \"audio\": 0,\r\n      \"cache_read\": 0\r\n    },\r\n    \"output_token_details\": {\r\n      \"audio\": 0,\r\n      \"reasoning\": 0\r\n    }\r\n  }\r\n}\n```\n\n<Note title=\"TIP\" type=\"success\">\n  If we've enabled LangSmith, we can see that this run is logged to LangSmith,\r\n  and can see the LangSmith trace. The LangSmith trace reports token usage\r\n  information, latency, standard model parameters (such as temperature), and\r\n  other information.\n</Note>\n\nNote that ChatModels receive message objects as input and generate message objects as output. In addition to text content, message objects convey conversational roles and hold important data, such as tool calls and token usage counts.\n\nLangChain also supports chat model inputs via strings or OpenAI format. The following are equivalent:\n\n```jsx {0} showLineNumbers\nawait model.invoke(\"Hello\");\r\n\r\nawait model.invoke([{ role: \"user\", content: \"Hello\" }]);\r\n\r\nawait model.invoke([new HumanMessage(\"hi!\")]);\n```\n\n### Streaming\n\nBecause chat models are Runnables, they expose a standard interface that includes async and streaming modes of invocation. This allows us to stream individual tokens from a chat model:\n\n```jsx {0} showLineNumbers\nconst stream = await model.stream(messages);\r\n\r\nconst chunks = [];\r\nfor await (const chunk of stream) {\r\n  chunks.push(chunk);\r\n  console.log(`${chunk.content}|`);\r\n}\n```\n\n```bash\n|\r\nC|\r\niao|\r\n!|\r\n|\r\n|\n```\n\n## Prompt Templates\n\nRight now we are passing a list of messages directly into the language model. Where does this list of messages come from? Usually, it is constructed from a combination of user input and application logic. This application logic usually takes the raw user input and transforms it into a list of messages ready to pass to the language model. Common transformations include adding a system message or formatting a template with the user input.\n\nPrompt templates are a concept in LangChain designed to assist with this transformation. They take in raw user input and return data (a prompt) that is ready to pass into a language model.\n\nLet’s create a prompt template here. It will take in two user variables:\n\n* **language**: The language to translate text into\n* **text**: The text to translate\n\n```jsx {0} showLineNumbers\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\n```\n\nFirst, let’s create a string that we will format to be the system message:\n\n```jsx {0} showLineNumbers\nconst systemTemplate = \"Translate the following from English into {language}\";\n```\n\nNext, we can create the PromptTemplate. This will be a combination of the systemTemplate as well as a simpler template for where to put the text\n\n```jsx {0} showLineNumbers\nconst promptTemplate = ChatPromptTemplate.fromMessages([\r\n  [\"system\", systemTemplate],\r\n  [\"user\", \"{text}\"],\r\n]);\n```\n\nNote that **ChatPromptTemplate** supports multiple message roles in a single template. We format the language parameter into the system message, and the user text into a user message.\n\nThe input to this prompt template is a dictionary. We can play around with this prompt template by itself to see what it does by itself\n\n```jsx {0} showLineNumbers\nconst promptValue = await promptTemplate.invoke({\r\n  language: \"italian\",\r\n  text: \"hi!\",\r\n});\r\n\r\npromptValue;\n```\n\n```bash\nChatPromptValue {\r\n  lc_serializable: true,\r\n  lc_kwargs: {\r\n    messages: [\r\n      SystemMessage {\r\n        \"content\": \"Translate the following from English into italian\",\r\n        \"additional_kwargs\": {},\r\n        \"response_metadata\": {}\r\n      },\r\n      HumanMessage {\r\n        \"content\": \"hi!\",\r\n        \"additional_kwargs\": {},\r\n        \"response_metadata\": {}\r\n      }\r\n    ]\r\n  },\r\n  lc_namespace: [ 'langchain_core', 'prompt_values' ],\r\n  messages: [\r\n    SystemMessage {\r\n      \"content\": \"Translate the following from English into italian\",\r\n      \"additional_kwargs\": {},\r\n      \"response_metadata\": {}\r\n    },\r\n    HumanMessage {\r\n      \"content\": \"hi!\",\r\n      \"additional_kwargs\": {},\r\n      \"response_metadata\": {}\r\n    }\r\n  ]\r\n}\n```\n\nWe can see that it returns a ChatPromptValue that consists of two messages. If we want to access the messages directly we do:\n\n```jsx {0} showLineNumbers\npromptValue.toChatMessages();\n```\n\n```bash\n[\r\n  SystemMessage {\r\n    \"content\": \"Translate the following from English into italian\",\r\n    \"additional_kwargs\": {},\r\n    \"response_metadata\": {}\r\n  },\r\n  HumanMessage {\r\n    \"content\": \"hi!\",\r\n    \"additional_kwargs\": {},\r\n    \"response_metadata\": {}\r\n  }\r\n]\n```\n\nFinally, we can invoke the chat model on the formatted prompt:\n\n```jsx {0} showLineNumbers\nconst response = await model.invoke(promptValue);\r\nconsole.log(`${response.content}`);\n```\n\n```bash\nCiao!\n```\n\nIf we take a look at the LangSmith trace, we can see all three components show up in the LangSmith trace.\n\n## Conclusion\n\nThat’s it! In this tutorial you’ve learned how to create your first simple LLM application. You’ve learned how to work with language models, how to create a prompt template, and how to get great observability into applications you create with LangSmith.\n\nThis just scratches the surface of what you will want to learn to become a proficient AI Engineer. Luckily - we’ve got a lot of other resources!\n\nFor further reading on the core concepts of LangChain, we’ve got detailed Conceptual Guides.\n\nIf you have more specific questions on these concepts, check out the following sections of the how-to guides:\n\n* [Chat models](/docs/how_to)\n* [Prompt templates](/docs/how_to)\n\nAnd the LangSmith docs:\n\n* [LangSmith](https://docs.smith.langchain.com/)\n"
  },
  {
    "slug": "/tutorials/retrievers",
    "title": "Build a semantic search engine",
    "description": "This tutorial will familiarize you with LangChain’s document loader, embedding, and vector store abstractions. These abstractions are designed to support retrieval of data– from (vector) databases and other sources– for integration with LLM workflows. They are important for applications that fetch data to be reasoned over as part of model inference, as in the case of retrieval-augmented generation, or RAG (see our RAG tutorial here).",
    "content": "Here we will build a search engine over a PDF document. This will allow us to retrieve passages in the PDF that are similar to an input query.\n\n## Concepts\n\nThis guide focuses on retrieval of text data. We will cover the following concepts:\n\n* Documents and document loaders;\n* Text splitters;\n* Embeddings;\n* Vector stores and retrievers.\n\n## Setup\n\n### Jupyter Notebook\n\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebook. See here for instructions on how to install.\n\n### Installation\n\nThis guide requires **@langchain/community** and **pdf-parse**:\n\nFor more details, see our [Installation guide](/docs/introduction/installation).\n\n### LangSmith\n\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\n\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\n\n```bash\nexport LANGCHAIN_TRACING_V2=\"true\"\r\nexport LANGCHAIN_API_KEY=\"...\"\r\n\r\n# Reduce tracing latency if you are not in a serverless environment\r\n# export LANGCHAIN_CALLBACKS_BACKGROUND=true\n```\n\n## Documents and Document Loaders\n\nLangChain implements a Document abstraction, which is intended to represent a unit of text and associated metadata. It has three attributes:\n\n* **pageContent**: a string representing the content;\n* **metadata**: records of arbitrary metadata;\n* **id**: (optional) a string identifier for the document.\n\nThe **metadata** attribute can capture information about the source of the document, its relationship to other documents, and other information. Note that an individual Document object often represents a chunk of a larger document.\n\nWe can generate sample documents when desired:\n\n```jsx {0} showLineNumbers\nimport { Document } from \"@langchain/core/documents\";\r\n\r\nconst documents = [\r\n  new Document({\r\n    pageContent:\r\n      \"Dogs are great companions, known for their loyalty and friendliness.\",\r\n    metadata: { source: \"mammal-pets-doc\" },\r\n  }),\r\n  new Document({\r\n    pageContent: \"Cats are independent pets that often enjoy their own space.\",\r\n    metadata: { source: \"mammal-pets-doc\" },\r\n  }),\r\n];\n```\n\nHowever, the LangChain ecosystem implements document loaders that integrate with hundreds of common sources. This makes it easy to incorporate data from these sources into your AI application.\n\n### Loading documents\n\nLet’s load a PDF into a sequence of Document objects. There is a sample PDF in the LangChain repo here – a 10-k filing for Nike from 2023. LangChain implements a PDFLoader that we can use to parse the PDF:\n\n```jsx {0} showLineNumbers\nimport { PDFLoader } from \"@langchain/community/document_loaders/fs/pdf\";\r\n\r\nconst loader = new PDFLoader(\"../../data/nke-10k-2023.pdf\");\r\n\r\nconst docs = await loader.load();\r\nconsole.log(docs.length);\n```\n\n```bash\n107\n```\n\n<Note title=\"TIP\" type=\"success\">\n  See [this guide](/docs/how_to/document_loader_pdf) for more detail on PDF document loaders.\n</Note>\n\n**PDFLoader** loads one Document object per PDF page. For each, we can easily access:\n\n* The string content of the page;\n* Metadata containing the file name and page number.\n\n```jsx {0} showLineNumbers\ndocs[0].pageContent.slice(0, 200);\n```\n\n```bash\nTable of Contents\r\nUNITED STATES\r\nSECURITIES AND EXCHANGE COMMISSION\r\nWashington, D.C. 20549\r\nFORM 10-K\r\n(Mark One)\r\n☑ ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(D) OF THE SECURITIES EXCHANGE ACT OF 1934\r\nFO\n```\n\n```jsx {0} showLineNumbers\ndocs[0].metadata;\n```\n\n```bash\n{\r\n  source: '../../data/nke-10k-2023.pdf',\r\n  pdf: {\r\n    version: '1.10.100',\r\n    info: {\r\n      PDFFormatVersion: '1.4',\r\n      IsAcroFormPresent: false,\r\n      IsXFAPresent: false,\r\n      Title: '0000320187-23-000039',\r\n      Author: 'EDGAR Online, a division of Donnelley Financial Solutions',\r\n      Subject: 'Form 10-K filed on 2023-07-20 for the period ending 2023-05-31',\r\n      Keywords: '0000320187-23-000039; ; 10-K',\r\n      Creator: 'EDGAR Filing HTML Converter',\r\n      Producer: 'EDGRpdf Service w/ EO.Pdf 22.0.40.0',\r\n      CreationDate: \"D:20230720162200-04'00'\",\r\n      ModDate: \"D:20230720162208-04'00'\"\r\n    },\r\n    metadata: null,\r\n    totalPages: 107\r\n  },\r\n  loc: { pageNumber: 1 }\r\n}\n```\n\n### Splitting\n\nFor both information retrieval and downstream question-answering purposes, a page may be too coarse a representation. Our goal in the end will be to retrieve Document objects that answer an input query, and further splitting our PDF will help ensure that the meanings of relevant portions of the document are not “washed out” by surrounding text.\n\nWe can use text splitters for this purpose. Here we will use a simple text splitter that partitions based on characters. We will split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. The overlap helps mitigate the possibility of separating a statement from important context related to it. We use the RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\n\nWe set **add\\_start\\_index=True** so that the character index where each split Document starts within the initial Document is preserved as metadata attribute “start\\_index”.\n\nSee [this guide](/docs/how_to/document_loader_pdf) for more detail about working with PDFs.\n\n```jsx {0} showLineNumbers\nimport { RecursiveCharacterTextSplitter } from \"@langchain/textsplitters\";\r\n\r\nconst textSplitter = new RecursiveCharacterTextSplitter({\r\n  chunkSize: 1000,\r\n  chunkOverlap: 200,\r\n});\r\n\r\nconst allSplits = await textSplitter.splitDocuments(docs);\r\n\r\nallSplits.length;\n```\n\n```bash\n513\n```\n\n## Embeddings\n\nVector search is a common way to store and search over unstructured data (such as unstructured text). The idea is to store numeric vectors that are associated with the text. Given a query, we can embed it as a vector of the same dimension and use vector similarity metrics (such as cosine similarity) to identify related text.\n\nLangChain supports embeddings from dozens of providers. These models specify how text should be converted into a numeric vector. Let’s select a model.\n\n```jsx {0} showLineNumbers\nconst vector1 = await embeddings.embedQuery(allSplits[0].pageContent);\r\nconst vector2 = await embeddings.embedQuery(allSplits[1].pageContent);\r\n\r\nconsole.assert(vector1.length === vector2.length);\r\nconsole.log(`Generated vectors of length ${vector1.length}\\n`);\r\nconsole.log(vector1.slice(0, 10));\n```\n\n```bash\nGenerated vectors of length 3072\r\n\r\n[\r\n  0.014310152,\r\n  -0.01681044,\r\n  -0.0011537228,\r\n  0.010546423,\r\n  0.022808468,\r\n  -0.028327717,\r\n  -0.00058849837,\r\n  0.0419197,\r\n  -0.0012900416,\r\n  0.0661778\r\n]\n```\n\nArmed with a model for generating text embeddings, we can next store them in a special data structure that supports efficient similarity search.\n\n## Vector stores\n\nLangChain VectorStore objects contain methods for adding text and Document objects to the store, and querying them using various similarity metrics. They are often initialized with embedding models, which determine how text data is translated to numeric vectors.\n\nLangChain includes a suite of integrations with different vector store technologies. Some vector stores are hosted by a provider (e.g., various cloud providers) and require specific credentials to use; some (such as Postgres) run in separate infrastructure that can be run locally or via a third-party; others can run in-memory for lightweight workloads.\n\n### Pick your vector store:\n\nHaving instantiated our vector store, we can now index the documents.\n\n```jsx {0} showLineNumbers\nawait vectorStore.addDocuments(allSplits);\n```\n\nNote that most vector store implementations will allow you to connect to an existing vector store– e.g., by providing a client, index name, or other information. See the documentation for a specific integration for more detail.\n\nOnce we’ve instantiated a **VectorStores** that contains documents, we can query it. VectorStore includes methods for querying: - Synchronously and asynchronously; - By string query and by vector; - With and without returning similarity scores; - By similarity and maximum marginal relevance (to balance similarity with query to diversity in retrieved results).\n\nThe methods will generally include a list of Document objects in their outputs.\n\n### Usage\n\nEmbeddings typically represent text as a “dense” vector such that texts with similar meanings are gemoetrically close. This lets us retrieve relevant information just by passing in a question, without knowledge of any specific key-terms used in the document.\n\nReturn documents based on similarity to a string query:\n\n```jsx {0} showLineNumbers\nconst results1 = await vectorStore.similaritySearch(\r\n  \"When was Nike incorporated?\"\r\n);\r\n\r\nresults1[0];\n```\n\n```bash\nDocument {\r\n  pageContent: 'Table of Contents\\n' +\r\n    'PART I\\n' +\r\n    'ITEM 1. BUSINESS\\n' +\r\n    'GENERAL\\n' +\r\n    'NIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\\n' +\r\n    '\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\\n' +\r\n    'Our principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\\n' +\r\n    'the largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\\n' +\r\n    'and sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales',\r\n  metadata: {\r\n    source: '../../data/nke-10k-2023.pdf',\r\n    pdf: {\r\n      version: '1.10.100',\r\n      info: [Object],\r\n      metadata: null,\r\n      totalPages: 107\r\n    },\r\n    loc: { pageNumber: 4, lines: [Object] }\r\n  },\r\n  id: undefined\r\n}\n```\n\nReturn scores:\n\n```jsx {0} showLineNumbers\nconst results2 = await vectorStore.similaritySearchWithScore(\r\n  \"What was Nike's revenue in 2023?\"\r\n);\r\n\r\nresults2[0];\n```\n\n```bash\n[\r\n  Document {\r\n    pageContent: 'Table of Contents\\n' +\r\n      'FISCAL 2023 NIKE BRAND REVENUE HIGHLIGHTS\\n' +\r\n      'The following tables present NIKE Brand revenues disaggregated by reportable operating segment, distribution channel and major product line:\\n' +\r\n      'FISCAL 2023 COMPARED TO FISCAL 2022\\n' +\r\n      '•NIKE, Inc. Revenues were $51.2 billion in fiscal 2023, which increased 10% and 16% compared to fiscal 2022 on a reported and currency-neutral basis, respectively.\\n' +\r\n      'The increase was due to higher revenues in North America, Europe, Middle East & Africa (\"EMEA\"), APLA and Greater China, which contributed approximately 7, 6,\\n' +\r\n      '2 and 1 percentage points to NIKE, Inc. Revenues, respectively.\\n' +\r\n      '•NIKE Brand revenues, which represented over 90% of NIKE, Inc. Revenues, increased 10% and 16% on a reported and currency-neutral basis, respectively. This\\n' +\r\n      \"increase was primarily due to higher revenues in Men's, the Jordan Brand, Women's and Kids' which grew 17%, 35%,11% and 10%, respectively, on a wholesale\\n\" +\r\n      'equivalent basis.',\r\n    metadata: {\r\n      source: '../../data/nke-10k-2023.pdf',\r\n      pdf: [Object],\r\n      loc: [Object]\r\n    },\r\n    id: undefined\r\n  },\r\n  0.6992287611800424\r\n]\n```\n\nReturn documents based on similarity to an embedded query:\n\n```jsx {0} showLineNumbers\nconst embedding = await embeddings.embedQuery(\r\n  \"How were Nike's margins impacted in 2023?\"\r\n);\r\n\r\nconst results3 = await vectorStore.similaritySearchVectorWithScore(\r\n  embedding,\r\n  1\r\n);\r\n\r\nresults3[0];\n```\n\n```bash\n[\r\n  Document {\r\n    pageContent: 'Table of Contents\\n' +\r\n      'GROSS MARGIN\\n' +\r\n      'FISCAL 2023 COMPARED TO FISCAL 2022\\n' +\r\n      'For fiscal 2023, our consolidated gross profit increased 4% to $22,292 million compared to $21,479 million for fiscal 2022. Gross margin decreased 250 basis points to\\n' +\r\n      '43.5% for fiscal 2023 compared to 46.0% for fiscal 2022 due to the following:\\n' +\r\n      '*Wholesale equivalent\\n' +\r\n      'The decrease in gross margin for fiscal 2023 was primarily due to:\\n' +\r\n      '•Higher NIKE Brand product costs, on a wholesale equivalent basis, primarily due to higher input costs and elevated inbound freight and logistics costs as well as\\n' +\r\n      'product mix;\\n' +\r\n      '•Lower margin in our NIKE Direct business, driven by higher promotional activity to liquidate inventory in the current period compared to lower promotional activity in\\n' +\r\n      'the prior period resulting from lower available inventory supply;\\n' +\r\n      '•Unfavorable changes in net foreign currency exchange rates, including hedges; and\\n' +\r\n      '•Lower off-price margin, on a wholesale equivalent basis.\\n' +\r\n      'This was partially offset by:',\r\n    metadata: {\r\n      source: '../../data/nke-10k-2023.pdf',\r\n      pdf: [Object],\r\n      loc: [Object]\r\n    },\r\n    id: undefined\r\n  },\r\n  0.7368815472158006\r\n]\n```\n\nLearn more:\n\n* [API reference](https://v03.api.js.langchain.com/classes/_langchain_core.vectorstores.VectorStore.html)\n* [How-to guide](/docs/how_to)\n* Integration-specific docs\n\n## Retrievers\n\nLangChain **VectorStore** objects do not subclass Runnable. LangChain Retrievers are Runnables, so they implement a standard set of methods (e.g., synchronous and asynchronous **invoke** and **batch** operations). Although we can construct retrievers from vector stores, retrievers can interface with non-vector store sources of data, as well (such as external APIs).\n\nVectorstores implement an as retriever method that will generate a Retriever, specifically a VectorStoreRetriever. These retrievers include specific **search\\_type** and **search\\_kwargs** attributes that identify what methods of the underlying vector store to call, and how to parameterize them.\n\n```jsx {0} showLineNumbers\nconst retriever = vectorStore.asRetriever({\r\n  searchType: \"mmr\",\r\n  searchKwargs: {\r\n    fetchK: 1,\r\n  },\r\n});\r\n\r\nawait retriever.batch([\r\n  \"When was Nike incorporated?\",\r\n  \"What was Nike's revenue in 2023?\",\r\n]);\n```\n\n```bash\n[\r\n  [\r\n    Document {\r\n      pageContent: 'Table of Contents\\n' +\r\n        'PART I\\n' +\r\n        'ITEM 1. BUSINESS\\n' +\r\n        'GENERAL\\n' +\r\n        'NIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\\n' +\r\n        '\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\\n' +\r\n        'Our principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\\n' +\r\n        'the largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\\n' +\r\n        'and sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales',\r\n      metadata: [Object],\r\n      id: undefined\r\n    }\r\n  ],\r\n  [\r\n    Document {\r\n      pageContent: 'Table of Contents\\n' +\r\n        'FISCAL 2023 NIKE BRAND REVENUE HIGHLIGHTS\\n' +\r\n        'The following tables present NIKE Brand revenues disaggregated by reportable operating segment, distribution channel and major product line:\\n' +\r\n        'FISCAL 2023 COMPARED TO FISCAL 2022\\n' +\r\n        '•NIKE, Inc. Revenues were $51.2 billion in fiscal 2023, which increased 10% and 16% compared to fiscal 2022 on a reported and currency-neutral basis, respectively.\\n' +\r\n        'The increase was due to higher revenues in North America, Europe, Middle East & Africa (\"EMEA\"), APLA and Greater China, which contributed approximately 7, 6,\\n' +\r\n        '2 and 1 percentage points to NIKE, Inc. Revenues, respectively.\\n' +\r\n        '•NIKE Brand revenues, which represented over 90% of NIKE, Inc. Revenues, increased 10% and 16% on a reported and currency-neutral basis, respectively. This\\n' +\r\n        \"increase was primarily due to higher revenues in Men's, the Jordan Brand, Women's and Kids' which grew 17%, 35%,11% and 10%, respectively, on a wholesale\\n\" +\r\n        'equivalent basis.',\r\n      metadata: [Object],\r\n      id: undefined\r\n    }\r\n  ]\r\n]\n```\n\nVectorStoreRetriever supports search types of \"similarity\" (default) and \"mmr\" (maximum marginal relevance, described above).\n\nRetrievers can easily be incorporated into more complex applications, such as retrieval-augmented generation (RAG) applications that combine a given question with retrieved context into a prompt for a LLM. To learn more about building such an application, check out the [**RAG tutorial**](/docs/tutorials/rag) tutorial.\n\n## Learn more:\n\nRetrieval strategies can be rich and complex. For example:\n\n* We can infer hard rules and filters from a query (e.g., “using documents published after 2020”);\n* We can return documents that are linked to the retrieved context in some way (e.g., via some document taxonomy);\n* We can generate multiple embeddings for each unit of context;\n* We can ensemble results from multiple retrievers;\n* We can assign weights to documents, e.g., to weigh recent documents higher.\n\nThe retrievers section of the how-to guides covers these and other built-in retrieval strategies.\n\nIt is also straightforward to extend the BaseRetriever class in order to implement custom retrievers. See our how-to guide here.\n\n## Next steps\n\nYou’ve now seen how to build a semantic search engine over a PDF document.\n\nFor more on document loaders:\n\n* Conceptual guide\n* How-to guides\n* Available integrations\n\nFor more on embeddings:\n\n* Conceptual guide\n* How-to guides\n* Available integrations\n\nFor more on vector stores:\n\n* Conceptual guide\n* How-to guides\n* Available integrations\n\nFor more on RAG, see:\n\n* Build a Retrieval Augmented Generation (RAG) App\n* Related how-to guides\n"
  }
]